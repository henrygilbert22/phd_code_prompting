{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\".bin\")\n",
    "\n",
    "DATA_PATH = \"data\"\n",
    "CODE_CONTEST_DATA_PATH = f\"{DATA_PATH}/code_contest_data/\"\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "os.makedirs(CODE_CONTEST_DATA_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Contest Problem Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "from domain.problems_d import ContestProblemD, ContestProblemSetD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "dask.config.set({'dataframe.query-planning': True})\n",
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_parquet(\"code_contests/data/*.parquet\").map_partitions(\n",
    "    lambda x: ContestProblemSetD.compressed_from_df(x),\n",
    "    meta={}\n",
    "    ).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, problem_set in enumerate(df):\n",
    "    f_name = f\"{CODE_CONTEST_DATA_PATH}/chunk_{i}.bin\"\n",
    "    with open(f_name, \"wb\") as f:\n",
    "        f.write(problem_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Filtered Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "from domain.problems_d import ContestProblemSetD, PatchedSolutionD\n",
    "\n",
    "reader = CompressedDomainFileDAO(CODE_CONTEST_DATA_PATH, ContestProblemSetD)\n",
    "problem_sets: List[ContestProblemSetD] = []\n",
    "for problem_set in reader.read():\n",
    "    problem_sets.append(problem_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from domain.problems_d import PatchedSolutionD, TestResultSetD\n",
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "\n",
    "BASE_EVAL_RESULTS_PATH = \"data/base_eval_results\"\n",
    "\n",
    "mocked_patched_solutions = [\n",
    "    PatchedSolutionD(\n",
    "        problem_id=problem.proto_id,\n",
    "        patched_solution=solution.solution,\n",
    "        solution_id=solution.proto_id,\n",
    "        prompt_id=\"prompt_id\",\n",
    "        model=\"MODEL_TYPE_GPT_4_TURBO\",\n",
    "        patched_response={})\n",
    "    for problem_set in problem_sets[:1]\n",
    "    for problem in problem_set.problems\n",
    "    for solution in problem.solutions[:1]\n",
    "]\n",
    "mocked_tests = [\n",
    "    test\n",
    "    for problem_set in problem_sets[:1]\n",
    "    for problem in problem_set.problems\n",
    "    for test in problem.public_tests[:1]\n",
    "]\n",
    "print(f\"Running {len(mocked_patched_solutions)} patched solutions on {len(mocked_tests)} tests\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from code_patching.solution_evaluator import eval_patched_solutions\n",
    "\n",
    "test_result_dao = CompressedDomainFileDAO(BASE_EVAL_RESULTS_PATH, TestResultSetD)\n",
    "test_result_sets = list(eval_patched_solutions(\n",
    "        problem_tests=mocked_tests,\n",
    "        patched_solutions=mocked_patched_solutions,\n",
    "        domain_writer=test_result_dao,\n",
    "        batch_size=5000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Patched Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Filtered Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "from domain.problems_d import ContestProblemSetD, PatchedSolutionD\n",
    "\n",
    "reader = CompressedDomainFileDAO(CODE_CONTEST_DATA_PATH, ContestProblemSetD)\n",
    "problem_sets: List[ContestProblemSetD] = []\n",
    "for problem_set in reader.read():\n",
    "    problem_sets.append(problem_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERED_DIR = \"data/filtered_code_contest_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "from domain.problems_d import ContestProblemSetD\n",
    "\n",
    "compressed_dao = CompressedDomainFileDAO(FILTERED_DIR, ContestProblemSetD)\n",
    "filtered_problem_sets = []\n",
    "for problem_set in problem_sets:\n",
    "    filtered_problems = []\n",
    "    for problem in problem_set.problems[:5]:\n",
    "        filtered_problem = dataclasses.replace(\n",
    "            problem,\n",
    "            solutions=problem.solutions[:3],\n",
    "            public_tests=problem.public_tests[:3],\n",
    "            incorrect_solutions=problem.incorrect_solutions[:3])\n",
    "        filtered_problems.append(filtered_problem)\n",
    "    filtered_problem_set = dataclasses.replace(problem_set, problems=filtered_problems)\n",
    "    filtered_problem_sets.append(filtered_problem_set)\n",
    "\n",
    "num_inc_sol = sum(\n",
    "    len(problem.incorrect_solutions) \n",
    "    for filtered_problem_set in filtered_problem_sets\n",
    "    for problem in filtered_problem_set.problems)    \n",
    "num_pub_tests = sum(len(problem.public_tests) \n",
    "                    for filtered_problem_set in filtered_problem_sets\n",
    "    for problem in filtered_problem_set.problems)\n",
    "print(f\"Filtered {len(filtered_problem_sets)} problem sets, {num_inc_sol} incorrect solutions, and {num_pub_tests} public tests\")\n",
    "\n",
    "compressed_dao.write(filtered_problem_sets)    \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Patched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "from domain.problems_d import ContestProblemSetD\n",
    "\n",
    "reader = CompressedDomainFileDAO(FILTERED_DIR, ContestProblemSetD)\n",
    "problem_sets = list(reader.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_inc_sol = sum(\n",
    "    len(problem.incorrect_solutions) \n",
    "    for problem_set in problem_sets\n",
    "    for problem in problem_set.problems\n",
    "    )    \n",
    "num_pub_tests = sum(len(problem.public_tests) \n",
    "    for problem_set in problem_sets\n",
    "    for problem in problem_set.problems)\n",
    "print(f\"Filtered {len(problem_sets)} problem sets, {num_inc_sol} incorrect solutions, and {num_pub_tests} public tests\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open(\".env.secret\", \"r\") as f:\n",
    "    for line in f:\n",
    "        key, value = line.strip().split(\"=\")\n",
    "        os.environ[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "from code_patching.prompts import PROMPTS\n",
    "from code_patching.solution_generator import generate_prompted_dataset\n",
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "from domain.problems_d import ContestProblemSetD, PatchedSolutionD, TestResultSetD, PatchedSolutionSetD\n",
    "import proto.patched_solutions_pb2 as ps_pb2\n",
    "from llm_handler.openai_handler import OpenAIHandler\n",
    "\n",
    "\n",
    "PROMPTED_DIR = \"data/patched_solutions\"\n",
    "MODELS = [ps_pb2.MODEL_TYPE_GPT_4_TURBO, ps_pb2.MODEL_TYPE_GPT_3_5_TURBO]\n",
    "openai_handler = OpenAIHandler()\n",
    "prompted_dao = CompressedDomainFileDAO(PROMPTED_DIR, PatchedSolutionSetD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_solution_sets = list(\n",
    "    generate_prompted_dataset(\n",
    "        contest_problems=problem_sets,\n",
    "        model_types=MODELS,\n",
    "        prompts=PROMPTS,\n",
    "        result_batch_size=100,\n",
    "        domain_reader=prompted_dao))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Patched Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from collections import defaultdict\n",
    "\n",
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "from domain.problems_d import PatchedSolutionSetD, ContestProblemSetD\n",
    "\n",
    "problem_test_cases = defaultdict(list)\n",
    "filtered_problems = CompressedDomainFileDAO(FILTERED_DIR, ContestProblemSetD)\n",
    "for problem_set in reader.read():\n",
    "    for problem in problem_set.problems:\n",
    "        problem_test_cases[problem.proto_id].extend(problem.public_tests)\n",
    "       \n",
    "problem_patched_solutions = defaultdict(list)\n",
    "prompted_dao = CompressedDomainFileDAO(PROMPTED_DIR, PatchedSolutionSetD)\n",
    "for patched_solution_set in prompted_dao.read():\n",
    "    for patched_solution in patched_solution_set.solutions:\n",
    "        problem_patched_solutions[patched_solution.problem_id].append(patched_solution)\n",
    "\n",
    "assert set(problem_test_cases.keys()) == set(problem_patched_solutions.keys()), \"Problem ids do not match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(problem_test_cases)} problems\")\n",
    "num_cases = 0\n",
    "case_args = []\n",
    "for problem_id, test_cases in problem_test_cases.items():\n",
    "    num_solutions = len(problem_patched_solutions[problem_id])\n",
    "    num_tests = len(test_cases)\n",
    "    print(f\"Problem {problem_id} has {num_solutions} solutions and {num_tests} test cases\")\n",
    "    num_cases += (num_tests * num_solutions)\n",
    "print(f\"Running {num_cases} test cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from code_patching.solution_evaluator import eval_patched_solutions\n",
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "from domain.problems_d import TestResultSetD, TestResultD\n",
    "\n",
    "\n",
    "PATCHED_EVAL_RESULTS_PATH = \"data/patched_eval_results\"\n",
    "test_result_dao = CompressedDomainFileDAO(PATCHED_EVAL_RESULTS_PATH, TestResultSetD)\n",
    "test_result_sets = list(eval_patched_solutions(\n",
    "        problem_tests=problem_test_cases,\n",
    "        patched_solutions=problem_patched_solutions,\n",
    "        domain_writer=test_result_dao,\n",
    "        process_batch_size=10,\n",
    "        batch_size=1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "test_results = [\n",
    "    test_result\n",
    "    for test_result_set in test_result_sets\n",
    "    for test_result in test_result_set.test_results]\n",
    "\n",
    "total_tests = len(test_results)\n",
    "problem_to_results: Dict[str, List[TestResultD]] = defaultdict(list)\n",
    "for test_result in test_results:\n",
    "    problem_to_results[test_result.problem_id].append(test_result)\n",
    "\n",
    "exception_pct = sum(1 for test_result in test_results if test_result.exception_info) / total_tests\n",
    "total_exception_pct = round(exception_pct*100, 2)\n",
    "exception_pct_by_problem: Dict[str, float] = {}\n",
    "for problem_id, results in problem_to_results.items():\n",
    "    expcetion_pct = sum(1 for result in results if result.exception_info) / len(results)\n",
    "    exception_pct_by_problem[problem_id] = round(expcetion_pct*100, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average exception rate: {total_exception_pct}%\")\n",
    "for problem_id, problem_exception_pct in exception_pct_by_problem.items():\n",
    "    print(f\"Problem {problem_id}\\n      Exception Rate - {problem_exception_pct}%\\n      {round((problem_exception_pct/total_exception_pct*100)-100, 2)}% delta to Avg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code_prompting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
