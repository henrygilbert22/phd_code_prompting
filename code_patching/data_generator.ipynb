{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\".bin\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "FILTERED_DIR = \"data/filtered_code_contest_data\"\n",
    "CODE_CONTEST_DATA_PATH = \"data/code_contest_data/\"\n",
    "PROMPTED_DIR = \"data/patched_solutions\"\n",
    "PATCHED_EVAL_RESULTS_PATH = \"data/patched_eval_results\"\n",
    "BASE_EVAL_RESULTS_PATH = \"data/eval_results\"\n",
    "\n",
    "os.makedirs(BASE_EVAL_RESULTS_PATH, exist_ok=True)\n",
    "os.makedirs(FILTERED_DIR, exist_ok=True)\n",
    "os.makedirs(PROMPTED_DIR, exist_ok=True)\n",
    "os.makedirs(PATCHED_EVAL_RESULTS_PATH, exist_ok=True)\n",
    "os.makedirs(CODE_CONTEST_DATA_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Contest Problem Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "dask.config.set({'dataframe.query-planning': True})\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from domain.problems_d import ContestProblemSetD\n",
    "\n",
    "df = dd.read_parquet(\"code_contests/data/*.parquet\").map_partitions(\n",
    "    lambda x: ContestProblemSetD.compressed_from_df(x),\n",
    "    meta={}\n",
    "    ).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, problem_set in enumerate(df):\n",
    "    f_name = f\"{CODE_CONTEST_DATA_PATH}/chunk_{i}.bin\"\n",
    "    with open(f_name, \"wb\") as f:\n",
    "        f.write(problem_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering Down Problem Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "from domain.problems_d import ContestProblemSetD\n",
    "\n",
    "reader = CompressedDomainFileDAO(CODE_CONTEST_DATA_PATH, ContestProblemSetD)\n",
    "problem_sets: List[ContestProblemSetD] = []\n",
    "for problem_set in reader.read():\n",
    "    problem_sets.append(problem_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "from domain.problems_d import ContestProblemSetD\n",
    "\n",
    "compressed_dao = CompressedDomainFileDAO(FILTERED_DIR, ContestProblemSetD)\n",
    "compressed_dao.clear_cache()\n",
    "filtered_problem_sets = []\n",
    "for problem_set in problem_sets:\n",
    "    filtered_problems = []\n",
    "    for problem in problem_set.problems[:5]:\n",
    "        filtered_problem = dataclasses.replace(\n",
    "            problem,\n",
    "            solutions=problem.solutions[:5],\n",
    "            public_tests=problem.public_tests[:5] + problem.private_tests[:5],\n",
    "            incorrect_solutions=problem.incorrect_solutions[:5])\n",
    "        filtered_problems.append(filtered_problem)\n",
    "    filtered_problem_set = dataclasses.replace(problem_set, problems=filtered_problems)\n",
    "    filtered_problem_sets.append(filtered_problem_set)\n",
    "\n",
    "num_inc_sol = sum(\n",
    "    len(problem.incorrect_solutions) \n",
    "    for filtered_problem_set in filtered_problem_sets\n",
    "    for problem in filtered_problem_set.problems)    \n",
    "num_pub_tests = sum(len(problem.public_tests) \n",
    "                    for filtered_problem_set in filtered_problem_sets\n",
    "    for problem in filtered_problem_set.problems)\n",
    "print(f\"Filtered {len(filtered_problem_sets)} problem sets, {num_inc_sol} incorrect solutions, and {num_pub_tests} public tests\")\n",
    "\n",
    "compressed_dao.write(filtered_problem_sets)    \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Patched Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "from domain.problems_d import ContestProblemSetD\n",
    "\n",
    "reader = CompressedDomainFileDAO(FILTERED_DIR, ContestProblemSetD)\n",
    "problem_sets = list(reader.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_inc_sol = sum(\n",
    "    len(problem.incorrect_solutions) \n",
    "    for problem_set in problem_sets\n",
    "    for problem in problem_set.problems\n",
    "    )    \n",
    "num_pub_tests = sum(len(problem.public_tests) \n",
    "    for problem_set in problem_sets\n",
    "    for problem in problem_set.problems)\n",
    "print(f\"Filtered {len(problem_sets)} problem sets, {num_inc_sol} incorrect solutions, and {num_pub_tests} public tests\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open(\".env.secret\", \"r\") as f:\n",
    "    for line in f:\n",
    "        key, value = line.strip().split(\"=\")\n",
    "        os.environ[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "from code_patching.prompts import PROMPTS\n",
    "from code_patching.solution_generator import generate_prompted_dataset\n",
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "from domain.problems_d import PatchedSolutionSetD\n",
    "import proto.patched_solutions_pb2 as ps_pb2\n",
    "from llm_handler.openai_handler import OpenAIHandler\n",
    "\n",
    "\n",
    "MODELS = [ps_pb2.MODEL_TYPE_GPT_4_TURBO, ps_pb2.MODEL_TYPE_GPT_3_5_TURBO]\n",
    "openai_handler = OpenAIHandler()\n",
    "prompted_dao = CompressedDomainFileDAO(PROMPTED_DIR, PatchedSolutionSetD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "generated_solution_sets = list(\n",
    "    generate_prompted_dataset(\n",
    "        contest_problems=problem_sets,\n",
    "        model_types=MODELS,\n",
    "        prompts=PROMPTS,\n",
    "        max_workers=100,\n",
    "        result_batch_size=100,\n",
    "        domain_reader=prompted_dao))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Patched Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from collections import defaultdict\n",
    "\n",
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "from domain.problems_d import PatchedSolutionSetD, ContestProblemSetD\n",
    "\n",
    "problem_test_cases = defaultdict(list)\n",
    "filtered_problems = CompressedDomainFileDAO(FILTERED_DIR, ContestProblemSetD)\n",
    "for problem_set in filtered_problems.read():\n",
    "    for problem in problem_set.problems:\n",
    "        problem_test_cases[problem.proto_id].extend(problem.public_tests)\n",
    "       \n",
    "problem_patched_solutions = defaultdict(list)\n",
    "prompted_dao = CompressedDomainFileDAO(PROMPTED_DIR, PatchedSolutionSetD)\n",
    "for patched_solution_set in prompted_dao.read():\n",
    "    for patched_solution in patched_solution_set.solutions:\n",
    "        problem_patched_solutions[patched_solution.problem_id].append(patched_solution)\n",
    "\n",
    "if diff := set(problem_test_cases.keys()).symmetric_difference(set(problem_patched_solutions.keys())):\n",
    "    raise ValueError(f\"Problem ids do not match: {diff}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from code_patching.solution_evaluator import eval_patched_solutions\n",
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "from domain.problems_d import TestResultSetD\n",
    "\n",
    "\n",
    "test_result_dao = CompressedDomainFileDAO(PATCHED_EVAL_RESULTS_PATH, TestResultSetD)\n",
    "test_result_sets = list(eval_patched_solutions(\n",
    "        problem_tests=problem_test_cases,\n",
    "        patched_solutions=problem_patched_solutions,\n",
    "        domain_writer=test_result_dao,\n",
    "        process_batch_size=10,\n",
    "        batch_size=1000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Correct Solution For Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from collections import defaultdict\n",
    "\n",
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "from domain.problems_d import ContestProblemSetD, PatchedSolutionD\n",
    "import proto.patched_solutions_pb2 as ps_pb2\n",
    "\n",
    "\n",
    "base_problem_test_cases = defaultdict(list)\n",
    "base_problem_solutions = defaultdict(list)\n",
    "filtered_problems = CompressedDomainFileDAO(FILTERED_DIR, ContestProblemSetD)\n",
    "for problem_set in filtered_problems.read():\n",
    "    for problem in problem_set.problems:\n",
    "        base_problem_test_cases[problem.proto_id].extend(problem.public_tests)\n",
    "        for solution in problem.solutions:        \n",
    "            base_solution = PatchedSolutionD(\n",
    "                problem_id=problem.proto_id,\n",
    "                patched_solution=solution.solution,\n",
    "                solution_id=solution.proto_id,\n",
    "                prompt_id=\"base_solution\",\n",
    "                model=ps_pb2.MODEL_TYPE_UNSPECIFIED,\n",
    "                patched_response={})\n",
    "            base_problem_solutions[problem.proto_id].append(base_solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from code_patching.solution_evaluator import eval_patched_solutions\n",
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "from domain.problems_d import TestResultSetD\n",
    "\n",
    "\n",
    "test_result_dao = CompressedDomainFileDAO(BASE_EVAL_RESULTS_PATH, TestResultSetD)\n",
    "test_result_sets = list(eval_patched_solutions(\n",
    "        problem_tests=base_problem_test_cases,\n",
    "        patched_solutions=base_problem_solutions,\n",
    "        domain_writer=test_result_dao,\n",
    "        process_batch_size=10,\n",
    "        batch_size=1000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code_prompting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
