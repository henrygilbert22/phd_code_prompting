{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\".bin\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "FILTERED_DIR = \"data/filtered_code_contest_data\"\n",
    "CODE_CONTEST_DATA_PATH = \"data/code_contest_data/\"\n",
    "PROMPTED_DIR = \"data/patched_solutions\"\n",
    "PATCHED_EVAL_RESULTS_PATH = \"data/patched_eval_results\"\n",
    "BASE_EVAL_RESULTS_PATH = \"data/eval_results\"\n",
    "OPENAI_CONFIG_PATH = \".env.secret\"\n",
    "\n",
    "os.makedirs(BASE_EVAL_RESULTS_PATH, exist_ok=True)\n",
    "os.makedirs(FILTERED_DIR, exist_ok=True)\n",
    "os.makedirs(PROMPTED_DIR, exist_ok=True)\n",
    "os.makedirs(PATCHED_EVAL_RESULTS_PATH, exist_ok=True)\n",
    "os.makedirs(CODE_CONTEST_DATA_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "from llm_handler.openai_handler import OpenAIHandler as openai_handler\n",
    "openai_handler.set_openai_api_key(OPENAI_CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Contest Problem Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "dask.config.set({'dataframe.query-planning': True})\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from domain.problems_d import ContestProblemSetD\n",
    "\n",
    "df = dd.read_parquet(\"code_contests/data/*.parquet\").map_partitions(\n",
    "    lambda x: ContestProblemSetD.compressed_from_df(x),\n",
    "    meta={}\n",
    "    ).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, problem_set in enumerate(df):\n",
    "    f_name = f\"{CODE_CONTEST_DATA_PATH}/chunk_{i}.bin\"\n",
    "    with open(f_name, \"wb\") as f:\n",
    "        f.write(problem_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering Down Problem Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "from domain.problems_d import ContestProblemSetD\n",
    "\n",
    "reader = CompressedDomainFileDAO(CODE_CONTEST_DATA_PATH, ContestProblemSetD)\n",
    "problem_sets: List[ContestProblemSetD] = []\n",
    "for problem_set in reader.read():\n",
    "    problem_sets.append(problem_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 41 problem sets, 946 incorrect solutions, and 1070 public tests\n"
     ]
    }
   ],
   "source": [
    "import dataclasses\n",
    "\n",
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "from domain.problems_d import ContestProblemSetD\n",
    "\n",
    "compressed_dao = CompressedDomainFileDAO(FILTERED_DIR, ContestProblemSetD)\n",
    "compressed_dao.clear_cache()\n",
    "filtered_problem_sets = []\n",
    "for problem_set in problem_sets:\n",
    "    filtered_problems = []\n",
    "    for problem in problem_set.problems[:5]:\n",
    "        filtered_problem = dataclasses.replace(\n",
    "            problem,\n",
    "            solutions=problem.solutions[:5],\n",
    "            public_tests=problem.public_tests[:5] + problem.private_tests[:5],\n",
    "            incorrect_solutions=problem.incorrect_solutions[:5])\n",
    "        filtered_problems.append(filtered_problem)\n",
    "    filtered_problem_set = dataclasses.replace(problem_set, problems=filtered_problems)\n",
    "    filtered_problem_sets.append(filtered_problem_set)\n",
    "\n",
    "num_inc_sol = sum(\n",
    "    len(problem.incorrect_solutions) \n",
    "    for filtered_problem_set in filtered_problem_sets\n",
    "    for problem in filtered_problem_set.problems)    \n",
    "num_pub_tests = sum(len(problem.public_tests) \n",
    "                    for filtered_problem_set in filtered_problem_sets\n",
    "    for problem in filtered_problem_set.problems)\n",
    "print(f\"Filtered {len(filtered_problem_sets)} problem sets, {num_inc_sol} incorrect solutions, and {num_pub_tests} public tests\")\n",
    "\n",
    "compressed_dao.write(filtered_problem_sets)    \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Patched Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "from domain.problems_d import ContestProblemSetD\n",
    "\n",
    "reader = CompressedDomainFileDAO(FILTERED_DIR, ContestProblemSetD)\n",
    "patched_problem_sets = list(reader.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 41 problem sets, 946 incorrect solutions, and 1070 public tests\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_inc_sol = sum(\n",
    "    len(problem.incorrect_solutions) \n",
    "    for problem_set in patched_problem_sets\n",
    "    for problem in problem_set.problems\n",
    "    )    \n",
    "num_pub_tests = sum(len(problem.public_tests) \n",
    "    for problem_set in patched_problem_sets\n",
    "    for problem in problem_set.problems)\n",
    "print(f\"Filtered {len(patched_problem_sets)} problem sets, {num_inc_sol} incorrect solutions, and {num_pub_tests} public tests\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "from code_patching.prompts import PROMPTS\n",
    "from code_patching.solution_generator import generate_prompted_dataset\n",
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "from domain.problems_d import PatchedSolutionSetD\n",
    "import proto.patched_solutions_pb2 as ps_pb2\n",
    "\n",
    "\n",
    "MODELS = [ps_pb2.MODEL_TYPE_GPT_4_TURBO, ps_pb2.MODEL_TYPE_GPT_3_5_TURBO]\n",
    "prompted_dao = CompressedDomainFileDAO(PROMPTED_DIR, PatchedSolutionSetD)\n",
    "DRY_RUN = False\n",
    "GEN_SOLUTIONS_MAX_WORKERS = 100\n",
    "GEN_SOLUTIONS_BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Generated new 9460 args\n",
      "WARNING:root:Read 95 files\n",
      "WARNING:root:Skipped 9460 already generated solutions\n",
      "WARNING:root:Generated 0 and\n",
      "Solutions: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "\n",
    "generated_solution_sets = list(\n",
    "    generate_prompted_dataset(\n",
    "        contest_problems=patched_problem_sets,\n",
    "        model_types=MODELS,\n",
    "        prompts=PROMPTS,\n",
    "        max_workers=GEN_SOLUTIONS_MAX_WORKERS,\n",
    "        result_batch_size=GEN_SOLUTIONS_BATCH_SIZE,\n",
    "        domain_reader=prompted_dao,\n",
    "        dry_run=DRY_RUN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Patched Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Read 41 files from data/filtered_code_contest_data\n",
      "WARNING:root:Read 95 files from data/patched_solutions\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from collections import defaultdict\n",
    "\n",
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "from domain.problems_d import PatchedSolutionSetD, ContestProblemSetD\n",
    "\n",
    "problem_test_cases = defaultdict(list)\n",
    "filtered_problems = CompressedDomainFileDAO(FILTERED_DIR, ContestProblemSetD)\n",
    "for problem_set in filtered_problems.read():\n",
    "    for problem in problem_set.problems:\n",
    "        problem_test_cases[problem.proto_id].extend(problem.public_tests)\n",
    "       \n",
    "problem_patched_solutions = defaultdict(list)\n",
    "prompted_dao = CompressedDomainFileDAO(PROMPTED_DIR, PatchedSolutionSetD)\n",
    "for patched_solution_set in prompted_dao.read():\n",
    "    for patched_solution in patched_solution_set.solutions:\n",
    "        problem_patched_solutions[patched_solution.problem_id].append(patched_solution)\n",
    "\n",
    "if diff := set(problem_test_cases.keys()).symmetric_difference(set(problem_patched_solutions.keys())):\n",
    "    raise ValueError(f\"Problem ids do not match: {diff}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Read 48 files from data/patched_eval_results\n",
      "WARNING:root:Skipped 39592 already executed tests\n",
      "WARNING:root:process_batch_size=10 batch_size=1000 - 990 batches - 9898 total tests\n",
      "Test Evals:  34%|███▎      | 3320/9898 [01:23<00:50, 129.80it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/GitHub/phd_code_prompting/code_patching/solution_evaluator.py:94\u001b[0m, in \u001b[0;36meval_patched_solutions\u001b[0;34m(problem_tests, patched_solutions, domain_writer, max_workers, process_batch_size, batch_size)\u001b[0m\n\u001b[1;32m     93\u001b[0m results: List[TestResultD] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures\u001b[38;5;241m.\u001b[39mas_completed(batch_futures):\n\u001b[1;32m     95\u001b[0m     test_results \u001b[38;5;241m=\u001b[39m future\u001b[38;5;241m.\u001b[39mresult()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/concurrent/futures/_base.py:245\u001b[0m, in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) futures unfinished\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    243\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[0;32m--> 245\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m waiter\u001b[38;5;241m.\u001b[39mlock:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdomain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproblems_d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TestResultSetD\n\u001b[1;32m      9\u001b[0m test_result_dao \u001b[38;5;241m=\u001b[39m CompressedDomainFileDAO(PATCHED_EVAL_RESULTS_PATH, TestResultSetD)\n\u001b[0;32m---> 10\u001b[0m test_result_sets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43meval_patched_solutions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproblem_tests\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproblem_test_cases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatched_solutions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproblem_patched_solutions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdomain_writer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_result_dao\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprocess_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/phd_code_prompting/code_patching/solution_evaluator.py:86\u001b[0m, in \u001b[0;36meval_patched_solutions\u001b[0;34m(problem_tests, patched_solutions, domain_writer, max_workers, process_batch_size, batch_size)\u001b[0m\n\u001b[1;32m     82\u001b[0m logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprocess_batch_size\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(process_batches)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m batches - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(arg_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m total tests\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m )\n\u001b[1;32m     85\u001b[0m results_pbar \u001b[38;5;241m=\u001b[39m tqdm\u001b[38;5;241m.\u001b[39mtqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(arg_list), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Evals\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m futures\u001b[38;5;241m.\u001b[39mProcessPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m     88\u001b[0m     batch_futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     89\u001b[0m         executor\u001b[38;5;241m.\u001b[39msubmit(compute_batch, process_batch)\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m process_batch \u001b[38;5;129;01min\u001b[39;00m process_batches\n\u001b[1;32m     91\u001b[0m     ]\n\u001b[1;32m     93\u001b[0m     results: List[TestResultD] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/concurrent/futures/_base.py:649\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/concurrent/futures/process.py:780\u001b[0m, in \u001b[0;36mProcessPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor_manager_thread_wakeup\u001b[38;5;241m.\u001b[39mwakeup()\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor_manager_thread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m wait:\n\u001b[0;32m--> 780\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_executor_manager_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;66;03m# To reduce the risk of opening too many files, remove references to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;66;03m# objects that use file descriptors.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor_manager_thread \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1096\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1117\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1118\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from code_patching.solution_evaluator import eval_patched_solutions\n",
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "from domain.problems_d import TestResultSetD\n",
    "\n",
    "\n",
    "test_result_dao = CompressedDomainFileDAO(PATCHED_EVAL_RESULTS_PATH, TestResultSetD)\n",
    "test_result_sets = list(eval_patched_solutions(\n",
    "        problem_tests=problem_test_cases,\n",
    "        patched_solutions=problem_patched_solutions,\n",
    "        domain_writer=test_result_dao,\n",
    "        process_batch_size=10,\n",
    "        batch_size=1000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Correct Solution For Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from collections import defaultdict\n",
    "\n",
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "from domain.problems_d import ContestProblemSetD, PatchedSolutionD\n",
    "import proto.patched_solutions_pb2 as ps_pb2\n",
    "\n",
    "\n",
    "base_problem_test_cases = defaultdict(list)\n",
    "base_problem_solutions = defaultdict(list)\n",
    "filtered_problems = CompressedDomainFileDAO(FILTERED_DIR, ContestProblemSetD)\n",
    "for problem_set in filtered_problems.read():\n",
    "    for problem in problem_set.problems:\n",
    "        base_problem_test_cases[problem.proto_id].extend(problem.public_tests)\n",
    "        for solution in problem.solutions:        \n",
    "            base_solution = PatchedSolutionD(\n",
    "                problem_id=problem.proto_id,\n",
    "                patched_solution=solution.solution,\n",
    "                solution_id=solution.proto_id,\n",
    "                prompt_id=\"base_solution\",\n",
    "                model=ps_pb2.MODEL_TYPE_UNSPECIFIED,\n",
    "                patched_response={})\n",
    "            base_problem_solutions[problem.proto_id].append(base_solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from code_patching.solution_evaluator import eval_patched_solutions\n",
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "from domain.problems_d import TestResultSetD\n",
    "\n",
    "\n",
    "test_result_dao = CompressedDomainFileDAO(BASE_EVAL_RESULTS_PATH, TestResultSetD)\n",
    "test_result_sets = list(eval_patched_solutions(\n",
    "        problem_tests=base_problem_test_cases,\n",
    "        patched_solutions=base_problem_solutions,\n",
    "        domain_writer=test_result_dao,\n",
    "        process_batch_size=10,\n",
    "        batch_size=1000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code_prompting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
