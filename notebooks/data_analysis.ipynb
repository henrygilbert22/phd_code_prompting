{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\".bin\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "FILTERED_DIR = \"data/filtered_code_contest_data\"\n",
    "CODE_CONTEST_DATA_PATH = \"data/code_contest_data\"\n",
    "PROMPTED_DIR = \"data/patched_solutions\"\n",
    "PATCHED_EVAL_RESULTS_PATH = \"data/patched_eval_results\"\n",
    "BASE_EVAL_RESULTS_PATH = \"data/eval_results\"\n",
    "\n",
    "GRAPH_DIR = \"data/graphs\"\n",
    "os.makedirs(GRAPH_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Data Dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "from domain.problems_d import TestResultSetD, ContestProblemSetD, ContestProblemSetD, PatchedSolutionSetD\n",
    "from code_patching.prompts import PROMPTS\n",
    "\n",
    "\n",
    "test_result_dao = CompressedDomainFileDAO(PATCHED_EVAL_RESULTS_PATH, TestResultSetD)\n",
    "test_result_sets = list(test_result_dao.read())\n",
    "test_results = [\n",
    "    test_result for test_result_set in test_result_sets \n",
    "    for test_result in test_result_set.test_results]\n",
    "logging.info(f\"Loaded {len(test_results)} test results\")\n",
    "\n",
    "base_result_dao = CompressedDomainFileDAO(BASE_EVAL_RESULTS_PATH, TestResultSetD)   \n",
    "base_result_sets = list(base_result_dao.read())\n",
    "base_results = [\n",
    "    test_result for test_result_set in base_result_sets \n",
    "    for test_result in test_result_set.test_results]\n",
    "logging.info(f\"Loaded {len(base_results)} base test results\")\n",
    "\n",
    "problem_dao = CompressedDomainFileDAO(FILTERED_DIR, ContestProblemSetD)\n",
    "problem_sets = list(problem_dao.read())\n",
    "problem_ds = [\n",
    "    problem for problem_set in problem_sets\n",
    "    for problem in problem_set.problems]\n",
    "logging.info(f\"Loaded {len(problem_ds)} problems\")\n",
    "\n",
    "patched_solution_dao = CompressedDomainFileDAO(PROMPTED_DIR, PatchedSolutionSetD)\n",
    "patched_solution_sets = list(patched_solution_dao.read())\n",
    "patched_solutions = {\n",
    "    patched_solution.proto_id: patched_solution\n",
    "    for patched_solution_set in patched_solution_sets\n",
    "    for patched_solution in patched_solution_set.solutions}\n",
    "logging.info(f\"Loaded {len(patched_solutions)} patched solutions\")\n",
    "\n",
    "patching_prompts = {\n",
    "    prompt.proto_id: prompt\n",
    "    for prompt in PROMPTS}\n",
    "logging.info(f\"Loaded {len(patching_prompts)} patching prompts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Problem ID Alignment\n",
    "result_problem_ids = set([test_result.problem_id for test_result in test_results])\n",
    "logging.info(f\"{len(result_problem_ids)} unique problems in test results\")\n",
    "problem_ids = set([problem.proto_id for problem in problem_ds])\n",
    "logging.info(f\"{len(problem_ids)} unique problems in problem set\")\n",
    "unified_problem_ids = result_problem_ids.union(problem_ids)\n",
    "logging.info(f\"{len(unified_problem_ids)} unique problems in both test results and problem set\")\n",
    "if result_problem_ids != problem_ids:\n",
    "    difference = result_problem_ids.symmetric_difference(problem_ids)\n",
    "    logging.warning(f\"{len(difference)} test results do not have a corresponding problem in the problem set.\")\n",
    "\n",
    "# Test ID Alignment\n",
    "result_test_ids = set([test_result.test_id for test_result in test_results])\n",
    "logging.info(f\"{len(result_test_ids)} unique tests in test results\")\n",
    "test_ids = set([test.proto_id for problem in problem_ds for test in problem.public_tests])\n",
    "logging.info(f\"{len(test_ids)} unique tests in problem set\")\n",
    "unified_test_ids = result_test_ids.union(test_ids)\n",
    "logging.info(f\"{len(unified_test_ids)} unique tests in both test results and problem set\")\n",
    "if result_test_ids != test_ids:\n",
    "    raise ValueError(f\"Test ids in test results and problem set do not match with {result_test_ids.symmetric_difference(test_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Union\n",
    "\n",
    "import proto.contest_problem_pb2 as cp_pb2\n",
    "import proto.patched_solutions_pb2 as ps_pb2\n",
    "from domain.problems_d import ContestProblemD, TestResultD\n",
    "from llm_handler.openai_handler import OpenAIHandler\n",
    "\n",
    "\n",
    "def difficulty_to_int(difficulty: int) -> float:\n",
    "    \"\"\" Translates to 1-20 scale for difficulty then quantizes to 0-1 float\"\"\" \n",
    "    DIFFICULTY_SCALER_MAP = {\n",
    "        cp_pb2.ContestProblem.Difficulty.UNKNOWN_DIFFICULTY: -1,  # to purposefully segregate unknown difficulties\n",
    "        cp_pb2.ContestProblem.Difficulty.EASY: 1,\n",
    "        cp_pb2.ContestProblem.Difficulty.MEDIUM: 10,\n",
    "        cp_pb2.ContestProblem.Difficulty.HARD: 15,\n",
    "        cp_pb2.ContestProblem.Difficulty.HARDER: 17,\n",
    "        cp_pb2.ContestProblem.Difficulty.HARDEST: 20,\n",
    "        cp_pb2.ContestProblem.Difficulty.A: 1,\n",
    "        cp_pb2.ContestProblem.Difficulty.B: 2,\n",
    "        cp_pb2.ContestProblem.Difficulty.C: 3,\n",
    "        cp_pb2.ContestProblem.Difficulty.D: 4,\n",
    "        cp_pb2.ContestProblem.Difficulty.E: 5,\n",
    "        cp_pb2.ContestProblem.Difficulty.F: 6,\n",
    "        cp_pb2.ContestProblem.Difficulty.G: 7,\n",
    "        cp_pb2.ContestProblem.Difficulty.H: 8,\n",
    "        cp_pb2.ContestProblem.Difficulty.I: 9,\n",
    "        cp_pb2.ContestProblem.Difficulty.J: 10,\n",
    "        cp_pb2.ContestProblem.Difficulty.K: 11,\n",
    "        cp_pb2.ContestProblem.Difficulty.L: 12,\n",
    "        cp_pb2.ContestProblem.Difficulty.M: 13,\n",
    "        cp_pb2.ContestProblem.Difficulty.N: 14,\n",
    "        cp_pb2.ContestProblem.Difficulty.O: 15,\n",
    "        cp_pb2.ContestProblem.Difficulty.P: 16,\n",
    "        cp_pb2.ContestProblem.Difficulty.Q: 16,\n",
    "        cp_pb2.ContestProblem.Difficulty.R: 17,\n",
    "        cp_pb2.ContestProblem.Difficulty.S: 17,\n",
    "        cp_pb2.ContestProblem.Difficulty.T: 18,\n",
    "        cp_pb2.ContestProblem.Difficulty.U: 19,\n",
    "        cp_pb2.ContestProblem.Difficulty.V: 20}\n",
    "    if difficulty not in DIFFICULTY_SCALER_MAP:\n",
    "        raise ValueError(f\"Unknown difficulty {difficulty}\")\n",
    "    diff_scaler = DIFFICULTY_SCALER_MAP[difficulty]\n",
    "    return diff_scaler / 20\n",
    "\n",
    "def output_transformer(test_output: str) -> str:\n",
    "    return str([int(char) for char in test_output if char.isdigit()])\n",
    "\n",
    "def problem_to_df_dict(problem: ContestProblemD) -> Dict[str, Any]:\n",
    "    difficulty = difficulty_to_int(problem.difficulty)\n",
    "    return {\n",
    "        \"problem_id\": problem.proto_id,\n",
    "        \"problem_name\": problem.name,\n",
    "        \"problem_difficulty\": problem.difficulty,\n",
    "        \"mapped_difficulty\": difficulty,\n",
    "        \"cf_points\": problem.cf_points,\n",
    "        \"cf_rating\": problem.cf_rating,\n",
    "        \"time_limit_nsec\": problem.time_limit_nsec,\n",
    "        \"memory_limit_bytes\": problem.memory_limit_bytes}\n",
    "\n",
    "def test_result_to_df_dict(result: TestResultD) -> Dict[str, Any]:\n",
    "    transformed_expected_output = output_transformer(result.expected_output)\n",
    "    transformed_solution_output = output_transformer(result.solution_output)\n",
    "    correct = transformed_expected_output == transformed_solution_output\n",
    "    return {\n",
    "            \"expected_output\": transformed_expected_output,\n",
    "            \"solution_output\": transformed_solution_output,\n",
    "            \"result_id\": result.proto_id,\n",
    "            \"test_id\": result.test_id,\n",
    "            \"solution_id\": result.solution_id,\n",
    "            \"correct\": int(correct),\n",
    "            \"failed\": int(bool(result.exception_info)),\n",
    "            \"exception_info\": result.exception_info}\n",
    "\n",
    "def model_name(model: Union[str, 'ps_pb2.ModelType']) -> str:\n",
    "    if model in OpenAIHandler._MODEL_NAME_TO_VERSION:\n",
    "        return OpenAIHandler._MODEL_NAME_TO_VERSION[model]\n",
    "    return str(model)\n",
    "    \n",
    "def format_prompt_name(prompt: str) -> str:\n",
    "    _PROMPT_NAME_MAP = {\n",
    "        'code_patching_prompt_base': \"Base Prompt\",\n",
    "        'code_patching_prompt_base_explanation': \"Change Explanation\",\n",
    "        \"code_patching_prompt_base_test_generation\": \"Test Generation\",\n",
    "        \"code_patching_prompt_self_evaluation\": \"Self Evaluation\",\n",
    "        \"code_patching_prompt_minimal\": \"Minimal Context\"\n",
    "    }\n",
    "    if prompt in _PROMPT_NAME_MAP:\n",
    "        return _PROMPT_NAME_MAP[prompt]\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from collections import defaultdict\n",
    "\n",
    "from domain.problems_d import TestResultD\n",
    "\n",
    "\n",
    "unified_result_dict: Dict[str, List[TestResultD]] = defaultdict(list)\n",
    "for test_result in test_results:\n",
    "    unified_result_dict[test_result.problem_id].append(test_result)\n",
    "\n",
    "base_unified_result_dict: Dict[str, List[TestResultD]] = defaultdict(list)\n",
    "for test_result in base_results:\n",
    "    base_unified_result_dict[test_result.problem_id].append(test_result)\n",
    "\n",
    "unified_problem_ds = [\n",
    "    problem for problem in problem_ds\n",
    "    if problem.proto_id in unified_problem_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any, List, Dict\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "unified_dict_records: List[Dict[str, Any]] = []\n",
    "for problem in unified_problem_ds:\n",
    "    patched_test_results = unified_result_dict[problem.proto_id]\n",
    "    base_test_results = base_unified_result_dict[problem.proto_id]\n",
    "    test_results = patched_test_results + base_test_results\n",
    "\n",
    "    difficulty = difficulty_to_int(problem.difficulty)\n",
    "    problem_dict = problem_to_df_dict(problem)\n",
    "    \n",
    "    \n",
    "    for result in test_results:\n",
    "        model = \"base_result\"\n",
    "        prompt_name = \"base_result\"\n",
    "        # required as base results exist in the same set but don't have model or prompt\n",
    "        if result.solution_id in patched_solutions: \n",
    "            solution = patched_solutions[result.solution_id]\n",
    "            model = solution.model\n",
    "            prompt_name = patching_prompts[solution.prompt_id].prompt_name\n",
    "        \n",
    "        test_dict = test_result_to_df_dict(result)\n",
    "        df_dict = {\n",
    "            **problem_dict, \n",
    "            **test_dict,\n",
    "            \"model\": model_name(model),\n",
    "            \"prompt_name\": format_prompt_name(prompt_name)}\n",
    "        unified_dict_records.append(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "RESULTS_DF = pd.DataFrame(unified_dict_records)\n",
    "logging.info(f\"Results DF: {RESULTS_DF.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from plotly import graph_objs as go\n",
    "\n",
    "def fig_handler(func, graph_dir: str = GRAPH_DIR):\n",
    "    def wrapper(*args, show: bool = True, save: bool=True, **kwargs):\n",
    "        fig = func(*args, **kwargs)\n",
    "        if not isinstance(fig, go.Figure):\n",
    "            raise ValueError(f\"Function {func.__name__} did not return a plotly figure\")\n",
    "       \n",
    "        if show: fig.show()    \n",
    "        if save:\n",
    "            file_name = str(fig.to_dict()[\"layout\"][\"title\"][\"text\"]).lower().replace(\" \", \"_\")\n",
    "            img_path = os.path.join(graph_dir, file_name) \n",
    "            fig.write_image(img_path + \".png\")\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "@fig_handler\n",
    "def model_type_performance(results_df: pd.DataFrame):\n",
    "    \n",
    "    correct_df = results_df[results_df['failed'] == 0] \n",
    "    correct_pct = correct_df.groupby('model')[['correct']].mean() * 100\n",
    "    failed_pct = results_df.groupby('model')['failed'].mean() * 100\n",
    "    \n",
    "    combined_df = pd.concat([correct_pct, failed_pct], axis=1)\n",
    "    combined_df.columns = [\"correct\", \"failed\"]\n",
    "    combined_df = combined_df.reset_index().sort_values(by=\"correct\", ascending=False)\n",
    "    fig = px.bar(\n",
    "        combined_df, \n",
    "        x='model',\n",
    "        y=\"correct\",\n",
    "        color='model',    \n",
    "        title=f\"Model Type Performance\",\n",
    "        labels={\"correct\": \"Correct (%)\"},\n",
    "        error_y=combined_df[\"failed\"]/2,\n",
    "        height=600,\n",
    "        width=800)\n",
    "    #  put text on error bars\n",
    "    for _, row in combined_df.iterrows():\n",
    "        fig.add_annotation(\n",
    "            x=row['model'], \n",
    "            y=row['correct'] + row['failed']/2 + 5, \n",
    "            text=f\"Â± {row['failed']:.2f}%\", \n",
    "            showarrow=False)\n",
    "    fig.update_layout(showlegend=False, title_x=0.5)\n",
    "    return fig\n",
    "\n",
    "model_type_performance(RESULTS_DF, show=True, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Type Performance Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model performance as distribution of scores across problems\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "\n",
    "@fig_handler\n",
    "def model_problem_performance_distribution(results_df: pd.DataFrame):\n",
    "    correct_df = results_df[results_df['failed'] == 0]\n",
    "    model_df = correct_df[correct_df['model'] != 'base_result']\n",
    "    model_problem_scores = model_df.groupby(['problem_id', 'model'])['correct'].mean().unstack().dropna()\n",
    "    dist_plot = ff.create_distplot(\n",
    "        [model_problem_scores[model] for model in model_problem_scores.columns],\n",
    "        model_problem_scores.columns,\n",
    "        show_hist=False, \n",
    "        show_rug=True,\n",
    "        bin_size=0.1,\n",
    "        )\n",
    "    dist_plot.update_layout(\n",
    "        title=\"Model Performance Distribution\",\n",
    "        xaxis_title=\"Correct (%)\",\n",
    "        yaxis_title=\"Density\",\n",
    "        height=600,\n",
    "        width=800)\n",
    "    return dist_plot\n",
    "\n",
    "model_problem_performance_distribution(RESULTS_DF, show=True, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Problem Performance Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model performance as distribution of scores across problems\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "\n",
    "@fig_handler\n",
    "def prompt_problem_performance_distribution(results_df: pd.DataFrame):\n",
    "    correct_df = results_df[results_df['failed'] == 0]\n",
    "    prompt_df = correct_df[correct_df['model'] != 'base_result']\n",
    "    prompt_problem_scores = prompt_df.groupby(['problem_id', 'prompt_name'])['correct'].mean().unstack().dropna()\n",
    "    dist_fig = ff.create_distplot(\n",
    "        [prompt_problem_scores[prompt] for prompt in prompt_problem_scores.columns],\n",
    "        prompt_problem_scores.columns,\n",
    "        show_hist=False, \n",
    "        show_rug=True,\n",
    "        )\n",
    "   \n",
    "    return dist_fig\n",
    "\n",
    "prompt_problem_performance_distribution(RESULTS_DF, show=True, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "@fig_handler\n",
    "def prompt_type_performance(results_df: pd.DataFrame):\n",
    "    \n",
    "    results_df = results_df[results_df['model'] != \"base_result\"]\n",
    "    correct_df = results_df[results_df['failed'] == 0] \n",
    "    correct_pct = correct_df.groupby('prompt_name')[['correct']].mean() * 100    \n",
    "    combined_df = correct_pct.reset_index().sort_values(by=\"correct\", ascending=False)\n",
    "    fig = px.bar(\n",
    "        combined_df, \n",
    "        x='prompt_name',\n",
    "        y=\"correct\",\n",
    "        color='prompt_name',    \n",
    "        title=f\"Prompt Type Performance\",\n",
    "        labels={\"correct\": \"Correct (%)\"},\n",
    "        height=600,\n",
    "        width=800)\n",
    "\n",
    "    fig.update_layout(showlegend=False, title_x=0.5)\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_type_performance(RESULTS_DF, show=True, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Imact Relative to Model Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fig_handler\n",
    "def prompt_performance_by_model(results_df: pd.DataFrame):\n",
    "    import seaborn as sns\n",
    "\n",
    "    results_df = results_df[results_df['model'] != \"base_result\"]\n",
    "    correct_df = results_df[results_df['failed'] == 0] \n",
    "    correct_pct = correct_df.groupby(['model', 'prompt_name'])[['correct']].mean() * 100    \n",
    "    combined_df = correct_pct.reset_index().sort_values(by=\"correct\", ascending=False)\n",
    "    \n",
    "    fig = px.bar(\n",
    "        combined_df, \n",
    "        x='model',\n",
    "        y=\"correct\",\n",
    "        color='prompt_name',\n",
    "        barmode='group',\n",
    "        title=f\"Prompt Performance by Model\",\n",
    "        labels={\"correct\": \"Correct (%)\"},\n",
    "        height=600,\n",
    "        width=800)\n",
    "    fig.update_layout(showlegend=True, title_x=0.5)\n",
    "    return fig\n",
    "\n",
    "prompt_performance_by_model(RESULTS_DF, show=True, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CF Rating Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mapped difficulty's impact on model performance\n",
    "@fig_handler\n",
    "def cf_rating_model_performance(results_df: pd.DataFrame):\n",
    "    correct_df = results_df[results_df['failed'] == 0]\n",
    "    cf_rating_df = correct_df[correct_df['cf_rating'] > 0]\n",
    "    difficulty_performance = cf_rating_df.groupby(['model', 'cf_rating'])['correct'].mean()\n",
    "    difficulty_sem = cf_rating_df.groupby(['model', 'cf_rating'])['correct'].sem()\n",
    "    difficulty_performance = difficulty_performance.reset_index()\n",
    "    difficulty_sem = difficulty_sem.reset_index()\n",
    "\n",
    "    #connect with a line\n",
    "    fig = px.scatter(\n",
    "        difficulty_performance, \n",
    "        x='cf_rating', \n",
    "        y='correct', \n",
    "        color='model',\n",
    "        color_continuous_scale=px.colors.sequential.Viridis,\n",
    "        error_y=difficulty_sem['correct'],\n",
    "        title=\"Model Performance by CF Rating\",\n",
    "        labels={\"cf_rating\": \"Codeforces Rating\", \"correct\": \"Correct (%)\"},\n",
    "        height=600,\n",
    "        width=800)\n",
    "    fig.update_traces(mode='markers+lines')\n",
    "    fig.update_layout(title_x=0.5)\n",
    "    return fig\n",
    "    \n",
    "cf_rating_model_performance(RESULTS_DF, show=True, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CF Rating Prompt Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fig_handler\n",
    "def cf_rating_prompt_performance(results_df: pd.DataFrame):\n",
    "    correct_df = results_df[\\\n",
    "        (results_df['failed'] == 0) & \\\n",
    "        (results_df['cf_rating'] > 0 )&\\\n",
    "        (results_df['model'] != \"base_result\")]\n",
    "\n",
    "    difficulty_performance = correct_df.groupby(['prompt_name', 'cf_rating'])['correct'].mean()\n",
    "    difficulty_performance = difficulty_performance.reset_index()\n",
    "\n",
    "    #connect with a line\n",
    "    fig = px.scatter(\n",
    "        difficulty_performance, \n",
    "        x='cf_rating', \n",
    "        y='correct', \n",
    "        color='prompt_name',\n",
    "        opacity=0.7,\n",
    "        color_continuous_scale=px.colors.sequential.Viridis,\n",
    "        title=\"Prompt Performance by CF Rating\",\n",
    "        labels={\"prompt_name\": \"CF Rating\", \"correct\": \"Correct (%)\"},\n",
    "        height=600,\n",
    "        width=800)\n",
    "    fig.update_layout(title_x=0.5)\n",
    "    # add average line \n",
    "    prompt_avg_correct = difficulty_performance.groupby('cf_rating')['correct'].mean()\n",
    "    fig.add_scatter(\n",
    "        x=prompt_avg_correct.index, \n",
    "        y=prompt_avg_correct.values, \n",
    "        mode='lines', \n",
    "        name='Average',\n",
    "        opacity=0.6,\n",
    "        line=dict(color='black', dash='dash'))\n",
    "    return fig\n",
    "\n",
    "cf_rating_prompt_performance(RESULTS_DF, show=True, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DF.drop(columns=['problem_id', '']).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@fig_handler\n",
    "def model_correlation_analysis(results_df: pd.DataFrame):\n",
    "    results_df = results_df[results_df['failed'] == 0]\n",
    "    results_df = results_df[results_df['cf_rating'] > 0]\n",
    "    results_df = results_df[results_df['model'] != \"base_result\"]\n",
    "\n",
    "    # plot heatmap of model correlation\n",
    "    model_corr = results_df.groupby('model')['correct'].mean().unstack()\n",
    "    model_corr = model_corr.corr()\n",
    "    fig = px.imshow(model_corr, title=\"Model Correlation Heatmap\")\n",
    "    return fig\n",
    "\n",
    "model_correlation_analysis(RESULTS_DF, show=True, save=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code_prompting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
