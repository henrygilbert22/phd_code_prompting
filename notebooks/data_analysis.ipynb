{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\".bin\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "FILTERED_DIR = \"data/filtered_code_contest_data\"\n",
    "CODE_CONTEST_DATA_PATH = \"data/code_contest_data\"\n",
    "PROMPTED_DIR = \"data/patched_solutions\"\n",
    "PATCHED_EVAL_RESULTS_PATH = \"data/patched_eval_results\"\n",
    "BASE_EVAL_RESULTS_PATH = \"data/eval_results\"\n",
    "\n",
    "GRAPH_DIR = \"data/graphs\"\n",
    "os.makedirs(GRAPH_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Data Dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "from domain.problems_d import TestResultSetD, ContestProblemSetD, ContestProblemSetD, PatchedSolutionSetD\n",
    "from code_patching.prompts import PROMPTS\n",
    "\n",
    "\n",
    "test_result_dao = CompressedDomainFileDAO(PATCHED_EVAL_RESULTS_PATH, TestResultSetD)\n",
    "test_result_sets = list(test_result_dao.read())\n",
    "test_results = [\n",
    "    test_result for test_result_set in test_result_sets \n",
    "    for test_result in test_result_set.test_results]\n",
    "logging.info(f\"Loaded {len(test_results)} test results\")\n",
    "\n",
    "base_result_dao = CompressedDomainFileDAO(BASE_EVAL_RESULTS_PATH, TestResultSetD)   \n",
    "base_result_sets = list(base_result_dao.read())\n",
    "base_results = [\n",
    "    test_result for test_result_set in base_result_sets \n",
    "    for test_result in test_result_set.test_results]\n",
    "logging.info(f\"Loaded {len(base_results)} base test results\")\n",
    "\n",
    "problem_dao = CompressedDomainFileDAO(FILTERED_DIR, ContestProblemSetD)\n",
    "problem_sets = list(problem_dao.read())\n",
    "problem_ds = [\n",
    "    problem for problem_set in problem_sets\n",
    "    for problem in problem_set.problems]\n",
    "logging.info(f\"Loaded {len(problem_ds)} problems\")\n",
    "\n",
    "patched_solution_dao = CompressedDomainFileDAO(PROMPTED_DIR, PatchedSolutionSetD)\n",
    "patched_solution_sets = list(patched_solution_dao.read())\n",
    "patched_solutions = {\n",
    "    patched_solution.proto_id: patched_solution\n",
    "    for patched_solution_set in patched_solution_sets\n",
    "    for patched_solution in patched_solution_set.solutions}\n",
    "logging.info(f\"Loaded {len(patched_solutions)} patched solutions\")\n",
    "\n",
    "patching_prompts = {\n",
    "    prompt.proto_id: prompt\n",
    "    for prompt in PROMPTS}\n",
    "logging.info(f\"Loaded {len(patching_prompts)} patching prompts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Problem ID Alignment\n",
    "result_problem_ids = set([test_result.problem_id for test_result in test_results])\n",
    "logging.info(f\"{len(result_problem_ids)} unique problems in test results\")\n",
    "problem_ids = set([problem.proto_id for problem in problem_ds])\n",
    "logging.info(f\"{len(problem_ids)} unique problems in problem set\")\n",
    "unified_problem_ids = result_problem_ids.union(problem_ids)\n",
    "logging.info(f\"{len(unified_problem_ids)} unique problems in both test results and problem set\")\n",
    "if result_problem_ids != problem_ids:\n",
    "    difference = result_problem_ids.symmetric_difference(problem_ids)\n",
    "    logging.warning(f\"{len(difference)} test results do not have a corresponding problem in the problem set.\")\n",
    "\n",
    "# Test ID Alignment\n",
    "result_test_ids = set([test_result.test_id for test_result in test_results])\n",
    "logging.info(f\"{len(result_test_ids)} unique tests in test results\")\n",
    "test_ids = set([test.proto_id for problem in problem_ds for test in problem.public_tests])\n",
    "logging.info(f\"{len(test_ids)} unique tests in problem set\")\n",
    "unified_test_ids = result_test_ids.union(test_ids)\n",
    "logging.info(f\"{len(unified_test_ids)} unique tests in both test results and problem set\")\n",
    "if result_test_ids != test_ids:\n",
    "    raise ValueError(f\"Test ids in test results and problem set do not match with {result_test_ids.symmetric_difference(test_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Union\n",
    "\n",
    "import proto.contest_problem_pb2 as cp_pb2\n",
    "import proto.patched_solutions_pb2 as ps_pb2\n",
    "from domain.problems_d import ContestProblemD, TestResultD\n",
    "from llm_handler.openai_handler import OpenAIHandler\n",
    "\n",
    "\n",
    "def difficulty_to_int(difficulty: int) -> float:\n",
    "    \"\"\" Translates to 1-20 scale for difficulty then quantizes to 0-1 float\"\"\" \n",
    "    DIFFICULTY_SCALER_MAP = {\n",
    "        cp_pb2.ContestProblem.Difficulty.UNKNOWN_DIFFICULTY: -1,  # to purposefully segregate unknown difficulties\n",
    "        cp_pb2.ContestProblem.Difficulty.EASY: 1,\n",
    "        cp_pb2.ContestProblem.Difficulty.MEDIUM: 10,\n",
    "        cp_pb2.ContestProblem.Difficulty.HARD: 15,\n",
    "        cp_pb2.ContestProblem.Difficulty.HARDER: 17,\n",
    "        cp_pb2.ContestProblem.Difficulty.HARDEST: 20,\n",
    "        cp_pb2.ContestProblem.Difficulty.A: 1,\n",
    "        cp_pb2.ContestProblem.Difficulty.B: 2,\n",
    "        cp_pb2.ContestProblem.Difficulty.C: 3,\n",
    "        cp_pb2.ContestProblem.Difficulty.D: 4,\n",
    "        cp_pb2.ContestProblem.Difficulty.E: 5,\n",
    "        cp_pb2.ContestProblem.Difficulty.F: 6,\n",
    "        cp_pb2.ContestProblem.Difficulty.G: 7,\n",
    "        cp_pb2.ContestProblem.Difficulty.H: 8,\n",
    "        cp_pb2.ContestProblem.Difficulty.I: 9,\n",
    "        cp_pb2.ContestProblem.Difficulty.J: 10,\n",
    "        cp_pb2.ContestProblem.Difficulty.K: 11,\n",
    "        cp_pb2.ContestProblem.Difficulty.L: 12,\n",
    "        cp_pb2.ContestProblem.Difficulty.M: 13,\n",
    "        cp_pb2.ContestProblem.Difficulty.N: 14,\n",
    "        cp_pb2.ContestProblem.Difficulty.O: 15,\n",
    "        cp_pb2.ContestProblem.Difficulty.P: 16,\n",
    "        cp_pb2.ContestProblem.Difficulty.Q: 16,\n",
    "        cp_pb2.ContestProblem.Difficulty.R: 17,\n",
    "        cp_pb2.ContestProblem.Difficulty.S: 17,\n",
    "        cp_pb2.ContestProblem.Difficulty.T: 18,\n",
    "        cp_pb2.ContestProblem.Difficulty.U: 19,\n",
    "        cp_pb2.ContestProblem.Difficulty.V: 20}\n",
    "    if difficulty not in DIFFICULTY_SCALER_MAP:\n",
    "        raise ValueError(f\"Unknown difficulty {difficulty}\")\n",
    "    diff_scaler = DIFFICULTY_SCALER_MAP[difficulty]\n",
    "    return diff_scaler / 20\n",
    "\n",
    "def output_transformer(test_output: str) -> str:\n",
    "    return str([int(char) for char in test_output if char.isdigit()])\n",
    "\n",
    "def problem_to_df_dict(problem: ContestProblemD) -> Dict[str, Any]:\n",
    "    difficulty = difficulty_to_int(problem.difficulty)\n",
    "    return {\n",
    "        \"problem_id\": problem.proto_id,\n",
    "        \"problem_name\": problem.name,\n",
    "        \"problem_difficulty\": problem.difficulty,\n",
    "        \"mapped_difficulty\": difficulty,\n",
    "        \"cf_points\": problem.cf_points,\n",
    "        \"cf_rating\": problem.cf_rating,\n",
    "        \"time_limit_nsec\": problem.time_limit_nsec,\n",
    "        \"memory_limit_bytes\": problem.memory_limit_bytes}\n",
    "\n",
    "def test_result_to_df_dict(result: TestResultD) -> Dict[str, Any]:\n",
    "    transformed_expected_output = output_transformer(result.expected_output)\n",
    "    transformed_solution_output = output_transformer(result.solution_output)\n",
    "    correct = transformed_expected_output == transformed_solution_output\n",
    "    return {\n",
    "            \"expected_output\": transformed_expected_output,\n",
    "            \"solution_output\": transformed_solution_output,\n",
    "            \"result_id\": result.proto_id,\n",
    "            \"test_id\": result.test_id,\n",
    "            \"solution_id\": result.solution_id,\n",
    "            \"correct\": int(correct),\n",
    "            \"failed\": int(bool(result.exception_info)),\n",
    "            \"exception_info\": result.exception_info}\n",
    "\n",
    "def model_name(model: Union[str, 'ps_pb2.ModelType']) -> str:\n",
    "    if model in OpenAIHandler._MODEL_NAME_TO_VERSION:\n",
    "        return OpenAIHandler._MODEL_NAME_TO_VERSION[model]\n",
    "    return str(model)\n",
    "    \n",
    "def format_prompt_name(prompt: str) -> str:\n",
    "    _PROMPT_NAME_MAP = {\n",
    "        'code_patching_prompt_base': \"Base Prompt\",\n",
    "        'code_patching_prompt_base_explanation': \"Change Explanation\",\n",
    "        \"code_patching_prompt_base_test_generation\": \"Test Generation\",\n",
    "        \"code_patching_prompt_self_evaluation\": \"Self Evaluation\",\n",
    "        \"code_patching_prompt_minimal\": \"Minimal Context\"\n",
    "    }\n",
    "    if prompt in _PROMPT_NAME_MAP:\n",
    "        return _PROMPT_NAME_MAP[prompt]\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from collections import defaultdict\n",
    "\n",
    "from domain.problems_d import TestResultD\n",
    "\n",
    "\n",
    "unified_result_dict: Dict[str, List[TestResultD]] = defaultdict(list)\n",
    "for test_result in test_results:\n",
    "    unified_result_dict[test_result.problem_id].append(test_result)\n",
    "\n",
    "base_unified_result_dict: Dict[str, List[TestResultD]] = defaultdict(list)\n",
    "for test_result in base_results:\n",
    "    base_unified_result_dict[test_result.problem_id].append(test_result)\n",
    "\n",
    "unified_problem_ds = [\n",
    "    problem for problem in problem_ds\n",
    "    if problem.proto_id in unified_problem_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any, List, Dict\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "unified_dict_records: List[Dict[str, Any]] = []\n",
    "for problem in unified_problem_ds:\n",
    "    patched_test_results = unified_result_dict[problem.proto_id]\n",
    "    base_test_results = base_unified_result_dict[problem.proto_id]\n",
    "    test_results = patched_test_results + base_test_results\n",
    "\n",
    "    difficulty = difficulty_to_int(problem.difficulty)\n",
    "    problem_dict = problem_to_df_dict(problem)\n",
    "    \n",
    "    \n",
    "    for result in test_results:\n",
    "        model = \"base_result\"\n",
    "        prompt_name = \"base_result\"\n",
    "        # required as base results exist in the same set but don't have model or prompt\n",
    "        if result.solution_id in patched_solutions: \n",
    "            solution = patched_solutions[result.solution_id]\n",
    "            model = solution.model\n",
    "            prompt_name = patching_prompts[solution.prompt_id].prompt_name\n",
    "        \n",
    "        test_dict = test_result_to_df_dict(result)\n",
    "        df_dict = {\n",
    "            **problem_dict, \n",
    "            **test_dict,\n",
    "            \"model\": model_name(model),\n",
    "            \"prompt_name\": format_prompt_name(prompt_name)}\n",
    "        unified_dict_records.append(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Results DF: (54154, 18)\n"
     ]
    }
   ],
   "source": [
    "RESULTS_DF = pd.DataFrame(unified_dict_records)\n",
    "logging.info(f\"Results DF: {RESULTS_DF.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from plotly import graph_objs as go\n",
    "\n",
    "def fig_handler(func, graph_dir: str = GRAPH_DIR):\n",
    "    def wrapper(*args, show: bool = True, save: bool=True, **kwargs):\n",
    "        fig = func(*args, **kwargs)\n",
    "        if not isinstance(fig, go.Figure):\n",
    "            raise ValueError(f\"Function {func.__name__} did not return a plotly figure\")\n",
    "       \n",
    "        if show: fig.show()    \n",
    "        if save:\n",
    "            file_name = str(fig.to_dict()[\"layout\"][\"title\"][\"text\"]).lower().replace(\" \", \"_\")\n",
    "            img_path = os.path.join(graph_dir, file_name) \n",
    "            fig.write_image(img_path + \".png\")\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "@fig_handler\n",
    "def model_type_performance(results_df: pd.DataFrame):\n",
    "    \n",
    "    correct_df = results_df[results_df['failed'] == 0] \n",
    "    correct_pct = correct_df.groupby('model')[['correct']].mean() * 100\n",
    "    failed_pct = results_df.groupby('model')['failed'].mean() * 100\n",
    "    \n",
    "    combined_df = pd.concat([correct_pct, failed_pct], axis=1)\n",
    "    combined_df.columns = [\"correct\", \"failed\"]\n",
    "    combined_df = combined_df.reset_index().sort_values(by=\"correct\", ascending=False)\n",
    "    fig = px.bar(\n",
    "        combined_df, \n",
    "        x='model',\n",
    "        y=\"correct\",\n",
    "        color='model',    \n",
    "        title=f\"Model Type Performance\",\n",
    "        labels={\"correct\": \"Correct (%)\"},\n",
    "        error_y=combined_df[\"failed\"]/2,\n",
    "        height=600,\n",
    "        width=800)\n",
    "    #  put text on error bars\n",
    "    for _, row in combined_df.iterrows():\n",
    "        fig.add_annotation(\n",
    "            x=row['model'], \n",
    "            y=row['correct'] + row['failed']/2 + 5, \n",
    "            text=f\"± {row['failed']:.2f}%\", \n",
    "            showarrow=False)\n",
    "    fig.update_layout(showlegend=False, title_x=0.5)\n",
    "    return fig\n",
    "\n",
    "model_type_performance(RESULTS_DF, show=True, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Type Performance Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model performance as distribution of scores across problems\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "\n",
    "@fig_handler\n",
    "def model_problem_performance_distribution(results_df: pd.DataFrame):\n",
    "    correct_df = results_df[results_df['failed'] == 0]\n",
    "    model_df = correct_df[correct_df['model'] != 'base_result']\n",
    "    model_problem_scores = model_df.groupby(['problem_id', 'model'])['correct'].mean().unstack().dropna()\n",
    "    dist_plot = ff.create_distplot(\n",
    "        [model_problem_scores[model] for model in model_problem_scores.columns],\n",
    "        model_problem_scores.columns,\n",
    "        show_hist=False, \n",
    "        show_rug=True,\n",
    "        bin_size=0.1,\n",
    "        )\n",
    "    dist_plot.update_layout(\n",
    "        title=\"Model Performance Distribution\",\n",
    "        xaxis_title=\"Correct (%)\",\n",
    "        yaxis_title=\"Density\",\n",
    "        height=600,\n",
    "        width=800)\n",
    "    return dist_plot\n",
    "\n",
    "model_problem_performance_distribution(RESULTS_DF, show=True, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Problem Performance Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model performance as distribution of scores across problems\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "\n",
    "@fig_handler\n",
    "def prompt_problem_performance_distribution(results_df: pd.DataFrame):\n",
    "    correct_df = results_df[results_df['failed'] == 0]\n",
    "    prompt_df = correct_df[correct_df['model'] != 'base_result']\n",
    "    prompt_problem_scores = prompt_df.groupby(['problem_id', 'prompt_name'])['correct'].mean().unstack().dropna()\n",
    "    dist_fig = ff.create_distplot(\n",
    "        [prompt_problem_scores[prompt] for prompt in prompt_problem_scores.columns],\n",
    "        prompt_problem_scores.columns,\n",
    "        show_hist=False, \n",
    "        show_rug=True,\n",
    "        )\n",
    "   \n",
    "    return dist_fig\n",
    "\n",
    "prompt_problem_performance_distribution(RESULTS_DF, show=True, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "@fig_handler\n",
    "def prompt_type_performance(results_df: pd.DataFrame):\n",
    "    \n",
    "    results_df = results_df[results_df['model'] != \"base_result\"]\n",
    "    correct_df = results_df[results_df['failed'] == 0] \n",
    "    correct_pct = correct_df.groupby('prompt_name')[['correct']].mean() * 100    \n",
    "    combined_df = correct_pct.reset_index().sort_values(by=\"correct\", ascending=False)\n",
    "    fig = px.bar(\n",
    "        combined_df, \n",
    "        x='prompt_name',\n",
    "        y=\"correct\",\n",
    "        color='prompt_name',    \n",
    "        title=f\"Prompt Type Performance\",\n",
    "        labels={\"correct\": \"Correct (%)\"},\n",
    "        height=600,\n",
    "        width=800)\n",
    "\n",
    "    fig.update_layout(showlegend=False, title_x=0.5)\n",
    "    return fig\n",
    "\n",
    "prompt_type_performance(RESULTS_DF, show=True, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Imact Relative to Model Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fig_handler\n",
    "def prompt_performance_by_model(results_df: pd.DataFrame):\n",
    "    import seaborn as sns\n",
    "\n",
    "    results_df = results_df[results_df['model'] != \"base_result\"]\n",
    "    correct_df = results_df[results_df['failed'] == 0] \n",
    "    correct_pct = correct_df.groupby(['model', 'prompt_name'])[['correct']].mean() * 100    \n",
    "    combined_df = correct_pct.reset_index().sort_values(by=\"correct\", ascending=False)\n",
    "    \n",
    "    fig = px.bar(\n",
    "        combined_df, \n",
    "        x='model',\n",
    "        y=\"correct\",\n",
    "        color='prompt_name',\n",
    "        barmode='group',\n",
    "        title=f\"Prompt Performance by Model\",\n",
    "        labels={\"correct\": \"Correct (%)\"},\n",
    "        height=600,\n",
    "        width=800)\n",
    "    fig.update_layout(showlegend=True, title_x=0.5)\n",
    "    return fig\n",
    "\n",
    "prompt_performance_by_model(RESULTS_DF, show=True, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CF Rating Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mapped difficulty's impact on model performance\n",
    "@fig_handler\n",
    "def cf_rating_model_performance(results_df: pd.DataFrame):\n",
    "    correct_df = results_df[results_df['failed'] == 0]\n",
    "    cf_rating_df = correct_df[correct_df['cf_rating'] > 0]\n",
    "    difficulty_performance = cf_rating_df.groupby(['model', 'cf_rating'])['correct'].mean()\n",
    "    difficulty_sem = cf_rating_df.groupby(['model', 'cf_rating'])['correct'].sem()\n",
    "    difficulty_performance = difficulty_performance.reset_index()\n",
    "    difficulty_sem = difficulty_sem.reset_index()\n",
    "\n",
    "    #connect with a line\n",
    "    fig = px.scatter(\n",
    "        difficulty_performance, \n",
    "        x='cf_rating', \n",
    "        y='correct', \n",
    "        color='model',\n",
    "        color_continuous_scale=px.colors.sequential.Viridis,\n",
    "        error_y=difficulty_sem['correct'],\n",
    "        title=\"Model Performance by CF Rating\",\n",
    "        labels={\"cf_rating\": \"Codeforces Rating\", \"correct\": \"Correct (%)\"},\n",
    "        height=600,\n",
    "        width=800)\n",
    "    fig.update_traces(mode='markers+lines')\n",
    "    fig.update_layout(title_x=0.5)\n",
    "    return fig\n",
    "    \n",
    "cf_rating_model_performance(RESULTS_DF, show=True, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CF Rating Prompt Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fig_handler\n",
    "def cf_rating_prompt_performance(results_df: pd.DataFrame):\n",
    "    correct_df = results_df[\\\n",
    "        (results_df['failed'] == 0) & \\\n",
    "        (results_df['cf_rating'] > 0 )&\\\n",
    "        (results_df['model'] != \"base_result\")]\n",
    "\n",
    "    difficulty_performance = correct_df.groupby(['prompt_name', 'cf_rating'])['correct'].mean()\n",
    "    difficulty_performance = difficulty_performance.reset_index()\n",
    "\n",
    "    #connect with a line\n",
    "    fig = px.scatter(\n",
    "        difficulty_performance, \n",
    "        x='cf_rating', \n",
    "        y='correct', \n",
    "        color='prompt_name',\n",
    "        opacity=0.7,\n",
    "        color_continuous_scale=px.colors.sequential.Viridis,\n",
    "        title=\"Prompt Performance by CF Rating\",\n",
    "        labels={\"prompt_name\": \"CF Rating\", \"correct\": \"Correct (%)\"},\n",
    "        height=600,\n",
    "        width=800)\n",
    "    fig.update_layout(title_x=0.5)\n",
    "    # add average line \n",
    "    prompt_avg_correct = difficulty_performance.groupby('cf_rating')['correct'].mean()\n",
    "    fig.add_scatter(\n",
    "        x=prompt_avg_correct.index, \n",
    "        y=prompt_avg_correct.values, \n",
    "        mode='lines', \n",
    "        name='Average',\n",
    "        opacity=0.6,\n",
    "        line=dict(color='black', dash='dash'))\n",
    "    return fig\n",
    "\n",
    "cf_rating_prompt_performance(RESULTS_DF, show=True, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot graph illustrating model and prompt impact on the rate of failure\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "\n",
    "@fig_handler\n",
    "def failure_rate_by_model(results_df: pd.DataFrame):\n",
    "    results_df = results_df[\\\n",
    "        (results_df['model'] != \"base_result\")]\n",
    "    \n",
    "    problem_failed_pct = results_df.groupby(['problem_id', 'prompt_name'])['failed'].mean().unstack().dropna()\n",
    "    hist_data = [problem_failed_pct[model] for model in problem_failed_pct.columns]\n",
    "    group_labels = problem_failed_pct.columns\n",
    "    \n",
    "    \n",
    "    fig = ff.create_distplot(\n",
    "        hist_data, \n",
    "        group_labels, \n",
    "        show_hist=False, \n",
    "        show_rug=True,\n",
    "        bin_size=0.05)\n",
    "    fig.update_layout(\n",
    "        title=\"Prompt Failure Distribution\",\n",
    "        xaxis_title=\"Correct (%)\",\n",
    "        yaxis_title=\"Density\",\n",
    "        height=600,\n",
    "        width=800)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "failure_rate_by_model(RESULTS_DF, show=True, save=False)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures as cf\n",
    "import tqdm\n",
    "from typing import List, Dict\n",
    "\n",
    "from llm_handler.openai_handler import OpenAIHandler\n",
    "OpenAIHandler.set_openai_api_key('.env.secret')\n",
    "\n",
    "\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    embedding = OpenAIHandler.get_text_embedding(text)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "\n",
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "from domain.problems_d import ContestProblemSetD\n",
    "\n",
    "\n",
    "problem_dao = CompressedDomainFileDAO(FILTERED_DIR, ContestProblemSetD)\n",
    "problem_sets = list(problem_dao.read())\n",
    "\n",
    "problem_ds = [\n",
    "    problem for problem_set in problem_sets\n",
    "    for problem in problem_set.problems]\n",
    "\n",
    "problem_id_to_description = {\n",
    "    problem.proto_id: problem.description\n",
    "    for problem in problem_ds}\n",
    "\n",
    "\n",
    "with cf.ThreadPoolExecutor() as executor:\n",
    "\n",
    "    future_map: Dict[cf.Future[List[float]], str] = {}\n",
    "    for problem in problem_ds:\n",
    "        embedding_future = executor.submit(get_embedding, problem.description)\n",
    "        future_map[embedding_future] = problem.proto_id\n",
    "\n",
    "    embeddings = {}\n",
    "    for future in tqdm.tqdm(cf.as_completed(future_map), total=len(future_map)):\n",
    "        problem_id = future_map[future]\n",
    "        embeddings[problem_id] = future.result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_results = RESULTS_DF[(RESULTS_DF['model'] != \"base_result\")].copy()\n",
    "# get average problem across all models and prompts\n",
    "QUANT_COLS = ['cf_rating', 'mapped_difficulty', 'cf_rating', 'time_limit_nsec', 'memory_limit_bytes', 'correct', 'failed']\n",
    "EVAL_DATA = filtered_results.groupby('problem_id')[QUANT_COLS].mean().reset_index().drop('problem_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Correct Base: 0.1779501103723018\n",
      "INFO:root:train score: 0.31614878717522066\n"
     ]
    }
   ],
   "source": [
    "# use gpc from sklearn to learn distribution of correct\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "problem_performance = EVAL_DATA.copy()\n",
    "problem_performance = problem_performance.drop('correct', axis=1)\n",
    "\n",
    "X = scaler.fit_transform(problem_performance)\n",
    "y = EVAL_DATA['correct']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "kernel = RBF() + WhiteKernel()\n",
    "gpc = GaussianProcessRegressor(kernel=kernel, random_state=42).fit(X_train, y_train)\n",
    "\n",
    "score=gpc.score(X_test, y_test) \n",
    "logging.info(f\"Correct Base: {score}\")\n",
    "logging.info(f\"train score: {gpc.score(X_train, y_train)}\")\n",
    "base_pred = gpc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_results = RESULTS_DF[(RESULTS_DF['model'] != \"base_result\")].copy()\n",
    "# get average problem across all models and prompts\n",
    "QUANT_COLS = ['cf_rating', 'mapped_difficulty', 'cf_rating', 'time_limit_nsec', 'memory_limit_bytes', 'correct']\n",
    "EVAL_DATA = filtered_results.groupby('problem_id')[QUANT_COLS].mean()\n",
    "\n",
    "EMBEDDING_DF = pd.DataFrame(embeddings).T\n",
    "# create df with embedding and correct frmo RESULTS_DF\n",
    "EMBEDDING_DF = EMBEDDING_DF.reset_index()\n",
    "EMBEDDING_DF.rename(columns={'index': 'problem_id'}, inplace=True)\n",
    "EMBEDDING_DF = EMBEDDING_DF.merge(EVAL_DATA['correct'], on='problem_id')\n",
    "\n",
    "Y = EMBEDDING_DF['correct']\n",
    "X = EMBEDDING_DF.drop(['problem_id', 'correct'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Correct Embedding Prediction Model Score: 0.22780370145169015\n",
      "INFO:root:Training Score: 0.6698477641747034\n"
     ]
    }
   ],
   "source": [
    "# use gpc from sklearn to learn distribution of correct\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "kernel = RBF() + WhiteKernel()\n",
    "gpc = GaussianProcessRegressor(kernel=kernel, random_state=42).fit(X_train, y_train)\n",
    "\n",
    "score = gpc.score(X_test, y_test)\n",
    "logging.info(f\"Correct Embedding Prediction Model Score: {score}\")\n",
    "logging.info(f\"Training Score: {gpc.score(X_train, y_train)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines",
         "name": "Embedding Trained Pred",
         "type": "scatter",
         "y": [
          0.5294949586696012,
          0.5101996833030125,
          0.4939214443661184,
          0.5198662599093531,
          0.47568576494389614,
          0.526568946749765,
          0.5170291902214528,
          0.36341182156299734,
          0.5471162576523358,
          0.577463774263764,
          0.7082728640731784,
          0.562265719880573,
          0.5146328109243719,
          0.45221004856778446,
          0.809295114866516,
          0.41410533075031086,
          0.4570397640995374,
          0.48547955731158865,
          0.5037150102470997,
          0.5229598002010256,
          0.5747226066059028,
          0.49377088022100946,
          0.70266122963538,
          0.4769950058354704,
          0.49748081386677523,
          0.6331086060038889,
          0.5534726915900903,
          0.5381735114144979,
          0.5924576884446529,
          0.3702212173490489,
          0.5664710739026217,
          0.46300554407859895,
          0.4139165056075438,
          0.5249848426366119,
          0.4594833243380796,
          0.3995417838307098,
          0.5073729794855568,
          0.5883517882636085,
          0.4174125700372935,
          0.5011617992975497,
          0.5523155795865904
         ]
        },
        {
         "mode": "lines",
         "name": "Base Trained Pred",
         "type": "scatter",
         "y": [
          0.5258220067116426,
          0.5318880890711029,
          0.49736211853871026,
          0.7321854265417258,
          0.528960127249448,
          0.39731136068199646,
          0.6082929371298036,
          0.41435112090812964,
          0.623826346261815,
          0.5225590570065677,
          0.25558765308622355,
          0.07293104274353368,
          0.33371283997874457,
          0.55650169136306,
          0.6522935253577771,
          0.674419012859417,
          0.2870730633238594,
          0.6301654689037335,
          0.4485549550771779,
          0.6221991483268354,
          0.5853110067672347,
          0.59577265441024,
          0.481467183031441,
          0.7332981555300222,
          0.5523228643256033,
          0.6043564103668615,
          0.6842479766046381,
          0.5160105830199129,
          0.37679453209375424,
          0.4202407706099791,
          0.6128462072635941,
          0.6797545097359148,
          0.5400469788328408,
          0.594385039409147,
          0.6138558537304633,
          0.6134260988272331,
          0.37908385583818216,
          0.5309987669685796,
          0.6262321972656864,
          0.5510490700488049,
          0.29452136014469943
         ]
        },
        {
         "mode": "markers",
         "name": "True",
         "type": "scatter",
         "y": [
          0.335,
          0.2642857142857143,
          0.72,
          0.418,
          0.38,
          0.5875,
          0.54,
          0.688,
          0.50625,
          0.56,
          0.58,
          0.48,
          0.195,
          0.38571428571428573,
          1,
          0.44,
          0.3,
          0.435,
          0.7622222222222222,
          0.36666666666666664,
          0.516,
          0.6675,
          1,
          0.48333333333333334,
          0.455,
          0.84,
          0.9,
          0.5028571428571429,
          0.44,
          0.31142857142857144,
          0.44,
          0.4275,
          0.3142857142857143,
          0.25666666666666665,
          0.5975,
          0.6114285714285714,
          0.14,
          1,
          0.5325,
          0.68,
          0.5771428571428572
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Correct Prediction Model"
        },
        "xaxis": {
         "title": {
          "text": "True Correct"
         }
        },
        "yaxis": {
         "title": {
          "text": "Predicted Correct"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot model performance as distribution of scores across problems\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "pred_line_vals = gpc.predict(X_test)\n",
    "fig.add_trace(go.Scatter(y=pred_line_vals, mode='lines', name='Embedding Trained Pred'))\n",
    "fig.add_trace(go.Scatter(y=base_pred, mode='lines', name='Base Trained Pred'))\n",
    "fig.add_trace(go.Scatter(y=y_test, mode='markers', name='True'))\n",
    "\n",
    "fig.update_layout(title=\"Correct Prediction Model\", xaxis_title=\"True Correct\", yaxis_title=\"Predicted Correct\")\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_results = RESULTS_DF[(RESULTS_DF['model'] != \"base_result\") & RESULTS_DF['failed'] == 0].copy()\n",
    "# get average problem across all models and prompts\n",
    "QUANT_COLS = ['cf_rating', 'mapped_difficulty', 'cf_rating', 'time_limit_nsec', 'memory_limit_bytes', 'correct', 'problem_id']\n",
    "EVAL_DATA = filtered_results[QUANT_COLS]\n",
    "\n",
    "EMBEDDING_DF = pd.DataFrame(embeddings).T\n",
    "EMBEDDING_DF = EMBEDDING_DF.reset_index().rename(columns={'index': 'problem_id'})\n",
    "EMBEDDING_DF = EMBEDDING_DF.merge(EVAL_DATA[['correct', 'problem_id']], on='problem_id')\n",
    "\n",
    "Y = EMBEDDING_DF['correct']\n",
    "X = EMBEDDING_DF.drop(['problem_id', 'correct'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GaussianProcessClassifier.__init__() got an unexpected keyword argument 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, Y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     10\u001b[0m kernel \u001b[38;5;241m=\u001b[39m RBF() \u001b[38;5;241m+\u001b[39m WhiteKernel()\n\u001b[0;32m---> 11\u001b[0m gpc \u001b[38;5;241m=\u001b[39m \u001b[43mGaussianProcessClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     13\u001b[0m score \u001b[38;5;241m=\u001b[39m gpc\u001b[38;5;241m.\u001b[39mscore(X_test, y_test)\n\u001b[1;32m     14\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed Embedding Prediction Model Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: GaussianProcessClassifier.__init__() got an unexpected keyword argument 'verbose'"
     ]
    }
   ],
   "source": [
    "# use gpc from sklearn to learn distribution of correct\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "kernel = RBF() + WhiteKernel()\n",
    "gpc = GaussianProcessClassifier(kernel=kernel, random_state=42, n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "score = gpc.score(X_test, y_test)\n",
    "logging.info(f\"Failed Embedding Prediction Model Score: {score}\")\n",
    "logging.info(f\"Training Score: {gpc.score(X_train, y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code_prompting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
