{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\".bin\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "FILTERED_DIR = \"data/filtered_code_contest_data\"\n",
    "CODE_CONTEST_DATA_PATH = \"data/code_contest_data\"\n",
    "PROMPTED_DIR = \"data/patched_solutions\"\n",
    "PATCHED_EVAL_RESULTS_PATH = \"data/patched_eval_results\"\n",
    "BASE_EVAL_RESULTS_PATH = \"data/eval_results\"\n",
    "\n",
    "GRAPH_DIR = \"data/graphs\"\n",
    "os.makedirs(GRAPH_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Data Dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "from domain.problems_d import TestResultSetD, ContestProblemSetD, ContestProblemSetD, PatchedSolutionSetD\n",
    "from code_patching.prompts import PROMPTS\n",
    "\n",
    "\n",
    "test_result_dao = CompressedDomainFileDAO(PATCHED_EVAL_RESULTS_PATH, TestResultSetD)\n",
    "test_result_sets = list(test_result_dao.read())\n",
    "test_results = [\n",
    "    test_result for test_result_set in test_result_sets \n",
    "    for test_result in test_result_set.test_results]\n",
    "logging.info(f\"Loaded {len(test_results)} test results\")\n",
    "\n",
    "base_result_dao = CompressedDomainFileDAO(BASE_EVAL_RESULTS_PATH, TestResultSetD)   \n",
    "base_result_sets = list(base_result_dao.read())\n",
    "base_results = [\n",
    "    test_result for test_result_set in base_result_sets \n",
    "    for test_result in test_result_set.test_results]\n",
    "logging.info(f\"Loaded {len(base_results)} base test results\")\n",
    "\n",
    "problem_dao = CompressedDomainFileDAO(FILTERED_DIR, ContestProblemSetD)\n",
    "problem_sets = list(problem_dao.read())\n",
    "problem_ds = [\n",
    "    problem for problem_set in problem_sets\n",
    "    for problem in problem_set.problems]\n",
    "logging.info(f\"Loaded {len(problem_ds)} problems\")\n",
    "\n",
    "patched_solution_dao = CompressedDomainFileDAO(PROMPTED_DIR, PatchedSolutionSetD)\n",
    "patched_solution_sets = list(patched_solution_dao.read())\n",
    "patched_solutions = {\n",
    "    patched_solution.proto_id: patched_solution\n",
    "    for patched_solution_set in patched_solution_sets\n",
    "    for patched_solution in patched_solution_set.solutions}\n",
    "logging.info(f\"Loaded {len(patched_solutions)} patched solutions\")\n",
    "\n",
    "patching_prompts = {\n",
    "    prompt.proto_id: prompt\n",
    "    for prompt in PROMPTS}\n",
    "logging.info(f\"Loaded {len(patching_prompts)} patching prompts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Problem ID Alignment\n",
    "result_problem_ids = set([test_result.problem_id for test_result in test_results])\n",
    "logging.info(f\"{len(result_problem_ids)} unique problems in test results\")\n",
    "problem_ids = set([problem.proto_id for problem in problem_ds])\n",
    "logging.info(f\"{len(problem_ids)} unique problems in problem set\")\n",
    "unified_problem_ids = result_problem_ids.union(problem_ids)\n",
    "logging.info(f\"{len(unified_problem_ids)} unique problems in both test results and problem set\")\n",
    "if result_problem_ids != problem_ids:\n",
    "    difference = result_problem_ids.symmetric_difference(problem_ids)\n",
    "    logging.warning(f\"{len(difference)} test results do not have a corresponding problem in the problem set.\")\n",
    "\n",
    "# Test ID Alignment\n",
    "result_test_ids = set([test_result.test_id for test_result in test_results])\n",
    "logging.info(f\"{len(result_test_ids)} unique tests in test results\")\n",
    "test_ids = set([test.proto_id for problem in problem_ds for test in problem.public_tests])\n",
    "logging.info(f\"{len(test_ids)} unique tests in problem set\")\n",
    "unified_test_ids = result_test_ids.union(test_ids)\n",
    "logging.info(f\"{len(unified_test_ids)} unique tests in both test results and problem set\")\n",
    "if result_test_ids != test_ids:\n",
    "    raise ValueError(f\"Test ids in test results and problem set do not match with {result_test_ids.symmetric_difference(test_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Union\n",
    "\n",
    "import proto.contest_problem_pb2 as cp_pb2\n",
    "import proto.patched_solutions_pb2 as ps_pb2\n",
    "from domain.problems_d import ContestProblemD, TestResultD\n",
    "from llm_handler.openai_handler import OpenAIHandler\n",
    "\n",
    "\n",
    "def difficulty_to_int(difficulty: int) -> float:\n",
    "    \"\"\" Translates to 1-20 scale for difficulty then quantizes to 0-1 float\"\"\" \n",
    "    DIFFICULTY_SCALER_MAP = {\n",
    "        cp_pb2.ContestProblem.Difficulty.UNKNOWN_DIFFICULTY: -1,  # to purposefully segregate unknown difficulties\n",
    "        cp_pb2.ContestProblem.Difficulty.EASY: 1,\n",
    "        cp_pb2.ContestProblem.Difficulty.MEDIUM: 10,\n",
    "        cp_pb2.ContestProblem.Difficulty.HARD: 15,\n",
    "        cp_pb2.ContestProblem.Difficulty.HARDER: 17,\n",
    "        cp_pb2.ContestProblem.Difficulty.HARDEST: 20,\n",
    "        cp_pb2.ContestProblem.Difficulty.A: 1,\n",
    "        cp_pb2.ContestProblem.Difficulty.B: 2,\n",
    "        cp_pb2.ContestProblem.Difficulty.C: 3,\n",
    "        cp_pb2.ContestProblem.Difficulty.D: 4,\n",
    "        cp_pb2.ContestProblem.Difficulty.E: 5,\n",
    "        cp_pb2.ContestProblem.Difficulty.F: 6,\n",
    "        cp_pb2.ContestProblem.Difficulty.G: 7,\n",
    "        cp_pb2.ContestProblem.Difficulty.H: 8,\n",
    "        cp_pb2.ContestProblem.Difficulty.I: 9,\n",
    "        cp_pb2.ContestProblem.Difficulty.J: 10,\n",
    "        cp_pb2.ContestProblem.Difficulty.K: 11,\n",
    "        cp_pb2.ContestProblem.Difficulty.L: 12,\n",
    "        cp_pb2.ContestProblem.Difficulty.M: 13,\n",
    "        cp_pb2.ContestProblem.Difficulty.N: 14,\n",
    "        cp_pb2.ContestProblem.Difficulty.O: 15,\n",
    "        cp_pb2.ContestProblem.Difficulty.P: 16,\n",
    "        cp_pb2.ContestProblem.Difficulty.Q: 16,\n",
    "        cp_pb2.ContestProblem.Difficulty.R: 17,\n",
    "        cp_pb2.ContestProblem.Difficulty.S: 17,\n",
    "        cp_pb2.ContestProblem.Difficulty.T: 18,\n",
    "        cp_pb2.ContestProblem.Difficulty.U: 19,\n",
    "        cp_pb2.ContestProblem.Difficulty.V: 20}\n",
    "    if difficulty not in DIFFICULTY_SCALER_MAP:\n",
    "        raise ValueError(f\"Unknown difficulty {difficulty}\")\n",
    "    diff_scaler = DIFFICULTY_SCALER_MAP[difficulty]\n",
    "    return diff_scaler / 20\n",
    "\n",
    "def output_transformer(test_output: str) -> str:\n",
    "    return str([int(char) for char in test_output if char.isdigit()])\n",
    "\n",
    "def problem_to_df_dict(problem: ContestProblemD) -> Dict[str, Any]:\n",
    "    difficulty = difficulty_to_int(problem.difficulty)\n",
    "    return {\n",
    "        \"problem_id\": problem.proto_id,\n",
    "        \"problem_name\": problem.name,\n",
    "        \"problem_difficulty\": problem.difficulty,\n",
    "        \"mapped_difficulty\": difficulty,\n",
    "        \"cf_points\": problem.cf_points,\n",
    "        \"cf_rating\": problem.cf_rating,\n",
    "        \"time_limit_nsec\": problem.time_limit_nsec,\n",
    "        \"memory_limit_bytes\": problem.memory_limit_bytes}\n",
    "\n",
    "def test_result_to_df_dict(result: TestResultD) -> Dict[str, Any]:\n",
    "    transformed_expected_output = output_transformer(result.expected_output)\n",
    "    transformed_solution_output = output_transformer(result.solution_output)\n",
    "    correct = transformed_expected_output == transformed_solution_output\n",
    "    return {\n",
    "            \"expected_output\": transformed_expected_output,\n",
    "            \"solution_output\": transformed_solution_output,\n",
    "            \"result_id\": result.proto_id,\n",
    "            \"test_id\": result.test_id,\n",
    "            \"solution_id\": result.solution_id,\n",
    "            \"correct\": int(correct),\n",
    "            \"failed\": int(bool(result.exception_info)),\n",
    "            \"exception_info\": result.exception_info}\n",
    "\n",
    "def model_name(model: Union[str, 'ps_pb2.ModelType']) -> str:\n",
    "    if model in OpenAIHandler._MODEL_NAME_TO_VERSION:\n",
    "        return OpenAIHandler._MODEL_NAME_TO_VERSION[model]\n",
    "    return str(model)\n",
    "    \n",
    "def format_prompt_name(prompt: str) -> str:\n",
    "    _PROMPT_NAME_MAP = {\n",
    "        'code_patching_prompt_base': \"Base Prompt\",\n",
    "        'code_patching_prompt_base_explanation': \"Change Explanation\",\n",
    "        \"code_patching_prompt_base_test_generation\": \"Test Generation\",\n",
    "        \"code_patching_prompt_self_evaluation\": \"Self Evaluation\",\n",
    "        \"code_patching_prompt_minimal\": \"Minimal Context\"\n",
    "    }\n",
    "    if prompt in _PROMPT_NAME_MAP:\n",
    "        return _PROMPT_NAME_MAP[prompt]\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from collections import defaultdict\n",
    "\n",
    "from domain.problems_d import TestResultD\n",
    "\n",
    "\n",
    "unified_result_dict: Dict[str, List[TestResultD]] = defaultdict(list)\n",
    "for test_result in test_results:\n",
    "    unified_result_dict[test_result.problem_id].append(test_result)\n",
    "\n",
    "base_unified_result_dict: Dict[str, List[TestResultD]] = defaultdict(list)\n",
    "for test_result in base_results:\n",
    "    base_unified_result_dict[test_result.problem_id].append(test_result)\n",
    "\n",
    "unified_problem_ds = [\n",
    "    problem for problem in problem_ds\n",
    "    if problem.proto_id in unified_problem_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any, List, Dict\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "unified_dict_records: List[Dict[str, Any]] = []\n",
    "for problem in unified_problem_ds:\n",
    "    patched_test_results = unified_result_dict[problem.proto_id]\n",
    "    base_test_results = base_unified_result_dict[problem.proto_id]\n",
    "    test_results = patched_test_results + base_test_results\n",
    "\n",
    "    difficulty = difficulty_to_int(problem.difficulty)\n",
    "    problem_dict = problem_to_df_dict(problem)\n",
    "    \n",
    "    \n",
    "    for result in test_results:\n",
    "        model = \"base_result\"\n",
    "        prompt_name = \"base_result\"\n",
    "        # required as base results exist in the same set but don't have model or prompt\n",
    "        if result.solution_id in patched_solutions: \n",
    "            solution = patched_solutions[result.solution_id]\n",
    "            model = solution.model\n",
    "            prompt_name = patching_prompts[solution.prompt_id].prompt_name\n",
    "        \n",
    "        test_dict = test_result_to_df_dict(result)\n",
    "        df_dict = {\n",
    "            **problem_dict, \n",
    "            **test_dict,\n",
    "            \"model\": model_name(model),\n",
    "            \"prompt_name\": format_prompt_name(prompt_name)}\n",
    "        unified_dict_records.append(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "RESULTS_DF = pd.DataFrame(unified_dict_records)\n",
    "logging.info(f\"Results DF: {RESULTS_DF.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from plotly import graph_objs as go\n",
    "\n",
    "def fig_handler(func, graph_dir: str = GRAPH_DIR):\n",
    "    def wrapper(*args, show: bool = True, save: bool=True, **kwargs):\n",
    "        fig = func(*args, **kwargs)\n",
    "        if not isinstance(fig, go.Figure):\n",
    "            raise ValueError(f\"Function {func.__name__} did not return a plotly figure\")\n",
    "       \n",
    "        if show: fig.show()    \n",
    "        if save:\n",
    "            file_name = str(fig.to_dict()[\"layout\"][\"title\"][\"text\"]).lower().replace(\" \", \"_\")\n",
    "            img_path = os.path.join(graph_dir, file_name) \n",
    "            fig.write_image(img_path + \".png\")\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "@fig_handler\n",
    "def model_type_performance(results_df: pd.DataFrame):\n",
    "    \n",
    "    correct_df = results_df[results_df['failed'] == 0] \n",
    "    correct_pct = correct_df.groupby('model')[['correct']].mean() * 100\n",
    "    failed_pct = results_df.groupby('model')['failed'].mean() * 100\n",
    "    \n",
    "    combined_df = pd.concat([correct_pct, failed_pct], axis=1)\n",
    "    combined_df.columns = [\"correct\", \"failed\"]\n",
    "    combined_df = combined_df.reset_index().sort_values(by=\"correct\", ascending=False)\n",
    "    fig = px.bar(\n",
    "        combined_df, \n",
    "        x='model',\n",
    "        y=\"correct\",\n",
    "        color='model',    \n",
    "        title=f\"Model Type Performance\",\n",
    "        labels={\"correct\": \"Correct (%)\"},\n",
    "        error_y=combined_df[\"failed\"]/2,\n",
    "        height=600,\n",
    "        width=800)\n",
    "    #  put text on error bars\n",
    "    for _, row in combined_df.iterrows():\n",
    "        fig.add_annotation(\n",
    "            x=row['model'], \n",
    "            y=row['correct'] + row['failed']/2 + 5, \n",
    "            text=f\"± {row['failed']:.2f}%\", \n",
    "            showarrow=False)\n",
    "    fig.update_layout(showlegend=False, title_x=0.5)\n",
    "    return fig\n",
    "\n",
    "model_type_performance(RESULTS_DF, show=True, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Type Performance Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model performance as distribution of scores across problems\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "\n",
    "@fig_handler\n",
    "def model_problem_performance_distribution(results_df: pd.DataFrame):\n",
    "    correct_df = results_df[results_df['failed'] == 0]\n",
    "    model_df = correct_df[correct_df['model'] != 'base_result']\n",
    "    model_problem_scores = model_df.groupby(['problem_id', 'model'])['correct'].mean().unstack().dropna()\n",
    "    dist_plot = ff.create_distplot(\n",
    "        [model_problem_scores[model] for model in model_problem_scores.columns],\n",
    "        model_problem_scores.columns,\n",
    "        show_hist=False, \n",
    "        show_rug=True,\n",
    "        bin_size=0.1,\n",
    "        )\n",
    "    dist_plot.update_layout(\n",
    "        title=\"Model Performance Distribution\",\n",
    "        xaxis_title=\"Correct (%)\",\n",
    "        yaxis_title=\"Density\",\n",
    "        height=600,\n",
    "        width=800)\n",
    "    return dist_plot\n",
    "\n",
    "model_problem_performance_distribution(RESULTS_DF, show=True, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Problem Performance Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model performance as distribution of scores across problems\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "\n",
    "@fig_handler\n",
    "def prompt_problem_performance_distribution(results_df: pd.DataFrame):\n",
    "    correct_df = results_df[results_df['failed'] == 0]\n",
    "    prompt_df = correct_df[correct_df['model'] != 'base_result']\n",
    "    prompt_problem_scores = prompt_df.groupby(['problem_id', 'prompt_name'])['correct'].mean().unstack().dropna()\n",
    "    dist_fig = ff.create_distplot(\n",
    "        [prompt_problem_scores[prompt] for prompt in prompt_problem_scores.columns],\n",
    "        prompt_problem_scores.columns,\n",
    "        show_hist=False, \n",
    "        show_rug=True,\n",
    "        )\n",
    "    dist_fig.update_layout(\n",
    "        title=\"Prompt Performance Distribution\",\n",
    "        xaxis_title=\"Correct (%)\",\n",
    "        yaxis_title=\"Density\",\n",
    "        height=600,\n",
    "        width=800)\n",
    "   \n",
    "    return dist_fig\n",
    "\n",
    "prompt_problem_performance_distribution(RESULTS_DF, show=True, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "@fig_handler\n",
    "def prompt_type_performance(results_df: pd.DataFrame):\n",
    "    \n",
    "    results_df = results_df[results_df['model'] != \"base_result\"]\n",
    "    correct_df = results_df[results_df['failed'] == 0] \n",
    "    correct_pct = correct_df.groupby('prompt_name')[['correct']].mean() * 100    \n",
    "    combined_df = correct_pct.reset_index().sort_values(by=\"correct\", ascending=False)\n",
    "    \n",
    "    # horizontal bar chart\n",
    "    fig = px.bar(\n",
    "        combined_df, \n",
    "        x='correct',\n",
    "        y=\"prompt_name\",\n",
    "        color='prompt_name',    \n",
    "        title=f\"Prompt Type Performance\",\n",
    "        labels={\"correct\": \"Correct (%)\"},\n",
    "        height=600,\n",
    "        width=800)\n",
    "\n",
    "    fig.update_layout(showlegend=True, title_x=0.5)\n",
    "    return fig\n",
    "\n",
    "prompt_type_performance(RESULTS_DF, show=True, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Imact Relative to Model Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fig_handler\n",
    "def prompt_performance_by_model(results_df: pd.DataFrame):\n",
    "    import seaborn as sns\n",
    "\n",
    "    correct_df = results_df[results_df['model'] != \"base_result\"]\n",
    "    correct_df = correct_df[correct_df['failed'] == 0] \n",
    "    correct_pct = correct_df.groupby(['model', 'prompt_name'])[['correct']].mean() * 100    \n",
    "    combined_df = correct_pct.reset_index().sort_values(by=\"correct\", ascending=False)\n",
    "    \n",
    "    base_prompt_scores = correct_df.groupby('model')['correct'].mean() * 100\n",
    "    base_prompt_scores = base_prompt_scores.reset_index()\n",
    "    model_diff = combined_df.merge(base_prompt_scores, on='model', suffixes=('', '_base'))\n",
    "    model_diff['performance_diff'] = model_diff['correct'] - model_diff['correct_base']\n",
    "    model_diff = model_diff.sort_values(by=\"performance_diff\", ascending=False)\n",
    "\n",
    "    fig = px.bar(\n",
    "        model_diff, \n",
    "        x='prompt_name',\n",
    "        y=\"performance_diff\",\n",
    "        color='model',  \n",
    "        barmode='group',  \n",
    "        title=f\"Impact of Prompt on Model Performance\",\n",
    "        labels={\"performance_diff\": \"Performance Difference (%)\"},\n",
    "        height=600,\n",
    "        width=800)\n",
    "    fig.update_layout(showlegend=True, title_x=0.5)\n",
    "    return fig\n",
    "\n",
    "prompt_performance_by_model(RESULTS_DF, show=True, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CF Rating Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mapped difficulty's impact on model performance\n",
    "@fig_handler\n",
    "def cf_rating_model_performance(results_df: pd.DataFrame):\n",
    "    correct_df = results_df[results_df['failed'] == 0]\n",
    "    cf_rating_df = correct_df[correct_df['cf_rating'] > 0]\n",
    "    difficulty_performance = cf_rating_df.groupby(['model', 'cf_rating'])['correct'].mean()\n",
    "    difficulty_sem = cf_rating_df.groupby(['model', 'cf_rating'])['correct'].sem()\n",
    "    difficulty_performance = difficulty_performance.reset_index()\n",
    "    difficulty_sem = difficulty_sem.reset_index()\n",
    "\n",
    "    #connect with a line\n",
    "    fig = px.scatter(\n",
    "        difficulty_performance, \n",
    "        x='cf_rating', \n",
    "        y='correct', \n",
    "        color='model',\n",
    "        color_continuous_scale=px.colors.sequential.Viridis,\n",
    "        error_y=difficulty_sem['correct'],\n",
    "        title=\"Model Performance by CF Rating\",\n",
    "        labels={\"cf_rating\": \"Codeforces Rating\", \"correct\": \"Correct (%)\"},\n",
    "        height=600,\n",
    "        width=800)\n",
    "    fig.update_traces(mode='markers+lines')\n",
    "    fig.update_layout(title_x=0.5)\n",
    "    return fig\n",
    "    \n",
    "cf_rating_model_performance(RESULTS_DF, show=True, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CF Rating Prompt Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fig_handler\n",
    "def cf_rating_prompt_performance(results_df: pd.DataFrame):\n",
    "    correct_df = results_df[\\\n",
    "        (results_df['failed'] == 0) & \\\n",
    "        (results_df['cf_rating'] > 0 )&\\\n",
    "        (results_df['model'] != \"base_result\")]\n",
    "\n",
    "    difficulty_performance = correct_df.groupby(['prompt_name', 'cf_rating'])['correct'].mean()\n",
    "    difficulty_performance = difficulty_performance.reset_index()\n",
    "\n",
    "    #connect with a line\n",
    "    fig = px.scatter(\n",
    "        difficulty_performance, \n",
    "        x='cf_rating', \n",
    "        y='correct', \n",
    "        color='prompt_name',\n",
    "        opacity=0.7,\n",
    "        color_continuous_scale=px.colors.sequential.Viridis,\n",
    "        title=\"Prompt Performance by CF Rating\",\n",
    "        labels={\"prompt_name\": \"CF Rating\", \"correct\": \"Correct (%)\"},\n",
    "        height=600,\n",
    "        width=800)\n",
    "    fig.update_layout(title_x=0.5)\n",
    "    # add average line \n",
    "    prompt_avg_correct = difficulty_performance.groupby('cf_rating')['correct'].mean()\n",
    "    fig.add_scatter(\n",
    "        x=prompt_avg_correct.index, \n",
    "        y=prompt_avg_correct.values, \n",
    "        mode='lines', \n",
    "        name='Average',\n",
    "        opacity=0.6,\n",
    "        line=dict(color='black', dash='dash'))\n",
    "    return fig\n",
    "\n",
    "cf_rating_prompt_performance(RESULTS_DF, show=True, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot graph illustrating model and prompt impact on the rate of failure\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "\n",
    "@fig_handler\n",
    "def failure_rate_by_model(results_df: pd.DataFrame):\n",
    "    results_df = results_df[\\\n",
    "        (results_df['model'] != \"base_result\")]\n",
    "    \n",
    "    problem_failed_pct = results_df.groupby(['problem_id', 'prompt_name'])['failed'].mean().unstack().dropna()\n",
    "    hist_data = [problem_failed_pct[model] for model in problem_failed_pct.columns]\n",
    "    group_labels = problem_failed_pct.columns\n",
    "    \n",
    "    \n",
    "    fig = ff.create_distplot(\n",
    "        hist_data, \n",
    "        group_labels, \n",
    "        show_hist=False, \n",
    "        show_rug=True,\n",
    "        bin_size=0.05)\n",
    "    fig.update_layout(\n",
    "        title=\"Prompt Failure Distribution\",\n",
    "        xaxis_title=\"Correct (%)\",\n",
    "        yaxis_title=\"Density\",\n",
    "        height=600,\n",
    "        width=800)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "failure_rate_by_model(RESULTS_DF, show=True, save=True)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures as cf\n",
    "import tqdm\n",
    "from typing import List, Dict\n",
    "\n",
    "from llm_handler.openai_handler import OpenAIHandler\n",
    "OpenAIHandler.set_openai_api_key('.env.secret')\n",
    "\n",
    "\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    embedding = OpenAIHandler.get_text_embedding(text)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "\n",
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "from domain.problems_d import ContestProblemSetD\n",
    "\n",
    "\n",
    "problem_dao = CompressedDomainFileDAO(FILTERED_DIR, ContestProblemSetD)\n",
    "problem_sets = list(problem_dao.read())\n",
    "\n",
    "problem_ds = [\n",
    "    problem for problem_set in problem_sets\n",
    "    for problem in problem_set.problems]\n",
    "\n",
    "problem_id_to_description = {\n",
    "    problem.proto_id: problem.description\n",
    "    for problem in problem_ds}\n",
    "\n",
    "\n",
    "with cf.ThreadPoolExecutor() as executor:\n",
    "\n",
    "    future_map: Dict[cf.Future[List[float]], str] = {}\n",
    "    for problem in problem_ds:\n",
    "        embedding_future = executor.submit(get_embedding, problem.description)\n",
    "        future_map[embedding_future] = problem.proto_id\n",
    "\n",
    "    embeddings = {}\n",
    "    for future in tqdm.tqdm(cf.as_completed(future_map), total=len(future_map)):\n",
    "        problem_id = future_map[future]\n",
    "        embeddings[problem_id] = future.result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_results = RESULTS_DF[(RESULTS_DF['model'] != \"base_result\")].copy()\n",
    "# get average problem across all models and prompts\n",
    "QUANT_COLS = ['cf_rating', 'mapped_difficulty', 'cf_rating', 'time_limit_nsec', 'memory_limit_bytes', 'correct', 'failed']\n",
    "EVAL_DATA = filtered_results.groupby('problem_id')[QUANT_COLS].mean().reset_index().drop('problem_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gpc from sklearn to learn distribution of correct\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "problem_performance = EVAL_DATA.copy()\n",
    "problem_performance = problem_performance.drop('correct', axis=1)\n",
    "\n",
    "X = scaler.fit_transform(problem_performance)\n",
    "y = EVAL_DATA['correct']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "kernel = RBF() + WhiteKernel()\n",
    "gpc = GaussianProcessRegressor(kernel=kernel, random_state=42).fit(X_train, y_train)\n",
    "\n",
    "score=gpc.score(X_test, y_test) \n",
    "logging.info(f\"Correct Base: {score}\")\n",
    "logging.info(f\"train score: {gpc.score(X_train, y_train)}\")\n",
    "base_pred = gpc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:PCA Components: 150\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DF = pd.DataFrame(embeddings).T\n",
    "\n",
    "# condense embeddings to 2D using PCA\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# get n_componenets to retain 95% of variance\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(EMBEDDING_DF)\n",
    "logging.info(f\"PCA Components: {pca.n_components_}\")\n",
    "\n",
    "embedding_2d = pca.transform(EMBEDDING_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_results = RESULTS_DF[(RESULTS_DF['model'] != \"base_result\")].copy()\n",
    "\n",
    "EMBEDDING_DF = pd.DataFrame(embedding_2d).T\n",
    "# create df with embedding and correct frmo RESULTS_DF\n",
    "EMBEDDING_DF = EMBEDDING_DF.reset_index().rename(columns={'index': 'problem_id'})\n",
    "\n",
    "mapping_df = filtered_results.groupby('problem_id')[QUANT_COLS].mean().reset_index()\n",
    "EMBEDDING_DF = EMBEDDING_DF.merge(mapping_df, on='problem_id')\n",
    "\n",
    "Y = EMBEDDING_DF['correct']\n",
    "X = EMBEDDING_DF.drop(['problem_id', 'correct', 'failed'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gpc from sklearn to learn distribution of correct\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "kernel = RBF() + WhiteKernel()\n",
    "gpc = GaussianProcessRegressor(kernel=kernel, random_state=42).fit(X_train, y_train)\n",
    "\n",
    "score = gpc.score(X_test, y_test)\n",
    "logging.info(f\"Correct Embedding Prediction Model Score: {score}\")\n",
    "logging.info(f\"Training Score: {gpc.score(X_train, y_train)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model performance as distribution of scores across problems\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "pred_line_vals = gpc.predict(X_test)\n",
    "fig.add_trace(go.Scatter(y=pred_line_vals, mode='lines', name='Embedding Trained Pred'))\n",
    "fig.add_trace(go.Scatter(y=base_pred, mode='lines', name='Base Trained Pred'))\n",
    "fig.add_trace(go.Scatter(y=y_test, mode='markers', name='True'))\n",
    "\n",
    "fig.update_layout(title=\"Correct Prediction Model\", xaxis_title=\"True Correct\", yaxis_title=\"Predicted Correct\")\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_results = RESULTS_DF[(RESULTS_DF['model'] != \"base_result\")].copy()\n",
    "# get average problem across all models and prompts\n",
    "QUANT_COLS = ['cf_rating', 'mapped_difficulty', 'cf_rating', 'time_limit_nsec', 'memory_limit_bytes', 'correct', 'failed']\n",
    "EVAL_DATA = filtered_results[QUANT_COLS]\n",
    "Y = EVAL_DATA['correct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(EVAL_DATA[EVAL_DATA['failed'] == 1]) / len(EVAL_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gpc from sklearn to learn distribution of correct\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X = scaler.fit_transform(EVAL_DATA.drop('correct', axis=1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "kernel = RBF() + WhiteKernel()\n",
    "gpc = GaussianProcessClassifier(kernel=kernel, random_state=42, n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "score = gpc.score(X_test, y_test)\n",
    "logging.info(f\"Failed Embedding Prediction Model Score: {score}\")\n",
    "logging.info(f\"Training Score: {gpc.score(X_train, y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get covariance matrix\n",
    "cov_matrix = gpc.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = np.argmax(cov_matrix, axis=1)\n",
    "con_matrix = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make bar plot of confusion matrix\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "true_negatives = con_matrix[0][0]\n",
    "false_negatives = con_matrix[1][0]\n",
    "true_positives = con_matrix[1][1]\n",
    "false_positives = con_matrix[0][1]\n",
    "cm_df = pd.DataFrame({\n",
    "    \"True Positive\": true_positives,\n",
    "    \"True Negative\": true_negatives,\n",
    "    \"False Positive\": false_positives,\n",
    "    \"False Negative\": false_negatives,\n",
    "    \n",
    "    }, index=[''])\n",
    "\n",
    "\n",
    "bar_fig = px.bar(\n",
    "    cm_df, \n",
    "    x=cm_df.index, \n",
    "    y=cm_df.columns, \n",
    "    barmode='group',\n",
    "    title=\"Confusion Matrix\",\n",
    "    labels={\"value\": \"Count\", \"index\": \"True\", \"variable\": \"Predicted\"})\n",
    "bar_fig.update_layout(title_x=0.5)\n",
    "bar_fig.write_image(\"data/graphs/confusion_matrix_gpc.png\")\n",
    "bar_fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(y_test == 1, 'correct', 'incorrect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# plot cdf of correct\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "\n",
    "pred_df = pd.DataFrame(cov_matrix, columns=['incorrect', 'correct'])\n",
    "pred_df['color'] = np.where(y_test == 1, 'correct', 'incorrect')\n",
    "\n",
    "fig = px.ecdf(pred_df, x=\"correct\",color='color', marginal=\"histogram\")\n",
    "fig.update_layout(title=\"CDF of Correct Predictions\", xaxis_title=\"Correct\", yaxis_title=\"CDF\")\n",
    "fig.show()\n",
    "fig.write_image(\"data/graphs/cdf_correct_pred_classifier.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph learned distribution of cov\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "# plot binomial distribution of cov_matrix\n",
    "# create CDF\n",
    "fig = ff.create_distplot([cov_matrix[:, 0], cov_matrix[:, 1]], group_labels=['incorrect', 'correct'], show_hist=True, show_rug=True, bin_size=0.05)\n",
    "fig.update_layout(title=\"Distribution of Posterior Probabilities\", xaxis_title=\"Correct\", yaxis_title=\"Density\")\n",
    "fig.write_image(\"data/graphs/pred_distribution_classifier.png\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code_prompting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
