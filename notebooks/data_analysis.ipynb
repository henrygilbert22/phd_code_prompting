{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\".bin\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "FILTERED_DIR = \"data/filtered_code_contest_data\"\n",
    "CODE_CONTEST_DATA_PATH = \"data/code_contest_data/\"\n",
    "PROMPTED_DIR = \"data/patched_solutions_v2\"\n",
    "PATCHED_EVAL_RESULTS_PATH = \"data/patched_eval_results\"\n",
    "BASE_EVAL_RESULTS_PATH = \"data/eval_results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Test Result and Problem Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from domain.domain_dao import CompressedDomainFileDAO\n",
    "from domain.problems_d import TestResultSetD, ContestProblemSetD, ContestProblemSetD, PatchedSolutionSetD, CodePatchingPromptD\n",
    "from code_patching.prompts import PROMPTS\n",
    "\n",
    "\n",
    "test_result_dao = CompressedDomainFileDAO(PATCHED_EVAL_RESULTS_PATH, TestResultSetD)\n",
    "test_result_sets = list(test_result_dao.read())\n",
    "test_results = [\n",
    "    test_result for test_result_set in test_result_sets \n",
    "    for test_result in test_result_set.test_results]\n",
    "\n",
    "base_result_dao = CompressedDomainFileDAO(BASE_EVAL_RESULTS_PATH, TestResultSetD)   \n",
    "base_result_sets = list(base_result_dao.read())\n",
    "base_results = [\n",
    "    test_result for test_result_set in base_result_sets \n",
    "    for test_result in test_result_set.test_results]\n",
    "\n",
    "problem_dao = CompressedDomainFileDAO(FILTERED_DIR, ContestProblemSetD)\n",
    "problem_sets = list(problem_dao.read())\n",
    "problem_ds = [\n",
    "    problem for problem_set in problem_sets\n",
    "    for problem in problem_set.problems]\n",
    "\n",
    "patched_solution_dao = CompressedDomainFileDAO(PROMPTED_DIR, PatchedSolutionSetD)\n",
    "patched_solution_sets = list(patched_solution_dao.read())\n",
    "patched_solutions = {\n",
    "    patched_solution.proto_id: patched_solution\n",
    "    for patched_solution_set in patched_solution_sets\n",
    "    for patched_solution in patched_solution_set.solutions}\n",
    "\n",
    "\n",
    "patching_prompts = {\n",
    "    prompt.proto_id: prompt\n",
    "    for prompt in PROMPTS}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Unified DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem ID Alignment\n",
    "result_problem_ids = set([test_result.problem_id for test_result in test_results])\n",
    "problem_ids = set([problem.proto_id for problem in problem_ds])\n",
    "unified_problem_ids = result_problem_ids.union(problem_ids)\n",
    "if result_problem_ids != problem_ids:\n",
    "    difference = result_problem_ids.symmetric_difference(problem_ids)\n",
    "    logging.warning(f\"Problem ids in test results and problem set do not match with {len(difference)}\\n {difference}\")\n",
    "\n",
    "# Test ID Alignment\n",
    "result_test_ids = set([test_result.test_id for test_result in test_results])\n",
    "test_ids = set([test.proto_id for problem in problem_ds for test in problem.public_tests])\n",
    "unified_test_ids = result_test_ids.union(test_ids)\n",
    "if result_test_ids != test_ids:\n",
    "    raise ValueError(f\"Test ids in test results and problem set do not match with {result_test_ids.symmetric_difference(test_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import proto.contest_problem_pb2 as cp_pb2\n",
    "\n",
    "def difficulty_to_int(difficulty: int) -> float:\n",
    "    \"\"\" Translates to 1-20 scale for difficulty then quantizes to 0-1 float\"\"\" \n",
    "    DIFFICULTY_SCALER_MAP = {\n",
    "        cp_pb2.ContestProblem.Difficulty.UNKNOWN_DIFFICULTY: -1,  # to purposefully segregate unknown difficulties\n",
    "        cp_pb2.ContestProblem.Difficulty.EASY: 1,\n",
    "        cp_pb2.ContestProblem.Difficulty.MEDIUM: 10,\n",
    "        cp_pb2.ContestProblem.Difficulty.HARD: 15,\n",
    "        cp_pb2.ContestProblem.Difficulty.HARDER: 17,\n",
    "        cp_pb2.ContestProblem.Difficulty.HARDEST: 20,\n",
    "        cp_pb2.ContestProblem.Difficulty.A: 1,\n",
    "        cp_pb2.ContestProblem.Difficulty.B: 2,\n",
    "        cp_pb2.ContestProblem.Difficulty.C: 3,\n",
    "        cp_pb2.ContestProblem.Difficulty.D: 4,\n",
    "        cp_pb2.ContestProblem.Difficulty.E: 5,\n",
    "        cp_pb2.ContestProblem.Difficulty.F: 6,\n",
    "        cp_pb2.ContestProblem.Difficulty.G: 7,\n",
    "        cp_pb2.ContestProblem.Difficulty.H: 8,\n",
    "        cp_pb2.ContestProblem.Difficulty.I: 9,\n",
    "        cp_pb2.ContestProblem.Difficulty.J: 10,\n",
    "        cp_pb2.ContestProblem.Difficulty.K: 11,\n",
    "        cp_pb2.ContestProblem.Difficulty.L: 12,\n",
    "        cp_pb2.ContestProblem.Difficulty.M: 13,\n",
    "        cp_pb2.ContestProblem.Difficulty.N: 14,\n",
    "        cp_pb2.ContestProblem.Difficulty.O: 15,\n",
    "        cp_pb2.ContestProblem.Difficulty.P: 16,\n",
    "        cp_pb2.ContestProblem.Difficulty.Q: 16,\n",
    "        cp_pb2.ContestProblem.Difficulty.R: 17,\n",
    "        cp_pb2.ContestProblem.Difficulty.S: 17,\n",
    "        cp_pb2.ContestProblem.Difficulty.T: 18,\n",
    "        cp_pb2.ContestProblem.Difficulty.U: 19,\n",
    "        cp_pb2.ContestProblem.Difficulty.V: 20}\n",
    "    if difficulty not in DIFFICULTY_SCALER_MAP:\n",
    "        raise ValueError(f\"Unknown difficulty {difficulty}\")\n",
    "    diff_scaler = DIFFICULTY_SCALER_MAP[difficulty]\n",
    "    return diff_scaler / 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from collections import defaultdict\n",
    "\n",
    "from domain.problems_d import TestResultD\n",
    "\n",
    "unified_result_dict: Dict[str, List[TestResultD]] = defaultdict(list)\n",
    "for test_result in test_results:\n",
    "    unified_result_dict[test_result.problem_id].append(test_result)\n",
    "\n",
    "base_unified_result_dict: Dict[str, List[TestResultD]] = defaultdict(list)\n",
    "for test_result in base_results:\n",
    "    base_unified_result_dict[test_result.problem_id].append(test_result)\n",
    "\n",
    "unified_problem_ds = [\n",
    "    problem for problem in problem_ds\n",
    "    if problem.proto_id in unified_problem_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Dict\n",
    "import pandas as pd\n",
    "\n",
    "import proto.patched_solutions_pb2 as ps_pb2\n",
    "\n",
    "\n",
    "unified_dict_records: List[Dict[str, Any]] = []\n",
    "for problem in unified_problem_ds:\n",
    "    patched_test_results = unified_result_dict[problem.proto_id]\n",
    "    base_test_results = base_unified_result_dict[problem.proto_id]\n",
    "    test_results = patched_test_results + base_test_results\n",
    "\n",
    "    difficulty = difficulty_to_int(problem.difficulty)\n",
    "    problem_dict = {\n",
    "        \"problem_id\": problem.proto_id,\n",
    "        \"problem_name\": problem.name,\n",
    "        \"problem_difficulty\": problem.difficulty,\n",
    "        \"mapped_difficulty\": difficulty,\n",
    "        \"cf_points\": problem.cf_points,\n",
    "        \"cf_rating\": problem.cf_rating,\n",
    "        \"time_limit_nsec\": problem.time_limit_nsec,\n",
    "        \"memory_limit_bytes\": problem.memory_limit_bytes}\n",
    "    \n",
    "    for result in test_results:\n",
    "        model = \"base_result\"\n",
    "        prompt_name = \"base_result\"\n",
    "        if result.solution_id in patched_solutions:\n",
    "            solution = patched_solutions[result.solution_id]\n",
    "            model = ps_pb2.ModelType.Name(solution.model)\n",
    "            prompt_name = patching_prompts[solution.prompt_id].prompt_name\n",
    "        \n",
    "        te_output = [int(char) for char in result.expected_output if char.isdigit()]\n",
    "        ts_output = [int(char) for char in result.solution_output if char.isdigit()]\n",
    "        ts_correct = te_output == ts_output\n",
    "        test_dict = {\n",
    "            \"expected_output\": result.expected_output,\n",
    "            \"solution_output\": result.solution_output,\n",
    "            'te_output': te_output,\n",
    "            'ts_output': ts_output,\n",
    "            \"ts_correct\": ts_correct,\n",
    "            \"result_id\": result.proto_id,\n",
    "            \"test_id\": result.test_id,\n",
    "            \"solution_id\": result.solution_id,\n",
    "            \"correct\": result.is_correct,\n",
    "            \"exception\": result.exception_info,\n",
    "            \"model\": model,\n",
    "            \"prompt_name\": prompt_name}\n",
    "        \n",
    "        unified_dict_records.append({**problem_dict, **test_dict})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_df = pd.DataFrame(unified_dict_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establishing Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df = unified_df[unified_df.model == \"base_result\"]\n",
    "gpt4_df = unified_df[unified_df.model == \"MODEL_TYPE_GPT_4_TURBO\"]\n",
    "gpt3_df = unified_df[unified_df.model == \"MODEL_TYPE_GPT_3_5_TURBO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_test_ids = set(gpt4_df['test_id'])\n",
    "gpt3_test_ids = set(gpt3_df['test_id'])\n",
    "baseline_test_ids = set(baseline_df['test_id'])\n",
    "\n",
    "assert gpt3_test_ids == gpt4_test_ids == baseline_test_ids\n",
    "gpt4_specific_test_ids = gpt4_test_ids - gpt3_test_ids - baseline_test_ids\n",
    "gpt3_specific_test_ids = gpt3_test_ids - gpt4_test_ids - baseline_test_ids\n",
    "baseline_specific_test_ids = baseline_test_ids - gpt4_test_ids - gpt3_test_ids\n",
    "\n",
    "print(f\"Baseline Specific Test Ids: {len(baseline_specific_test_ids)}\")\n",
    "print(f\"GPT3 Specific Test Ids: {len(gpt3_specific_test_ids)}\")\n",
    "print(f\"GPT4 Specific Test Ids: {len(gpt4_specific_test_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_avg_correct = gpt4_df['correct'].mean()\n",
    "gpt3_avg_correct = gpt3_df['correct'].mean()\n",
    "baseline_avg_correct = baseline_df['correct'].mean()\n",
    "\n",
    "print(f\"Baseline Average Correct: {baseline_avg_correct}\")\n",
    "print(f\"GPT3 Average Correct: {gpt3_avg_correct}\")\n",
    "print(f\"GPT4 Average Correct: {gpt4_avg_correct}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_avg_correct = gpt4_df['ts_correct'].mean()\n",
    "gpt3_avg_correct = gpt3_df['ts_correct'].mean()\n",
    "baseline_avg_correct = baseline_df['ts_correct'].mean()\n",
    "\n",
    "print(f\"Trans Baseline Average Correct: {baseline_avg_correct}\")\n",
    "print(f\"Trans GPT3 Average Correct: {gpt3_avg_correct}\")\n",
    "print(f\"Trans GPT4 Average Correct: {gpt4_avg_correct}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_non_exception_cnt = gpt4_df['exception'].value_counts()['']\n",
    "gpt3_non_exception_cnt = gpt3_df['exception'].value_counts()['']\n",
    "baseline_non_exception_cnt = baseline_df['exception'].value_counts()['']\n",
    "\n",
    "gpt4_exception_rate = (len(gpt4_df) - gpt4_non_exception_cnt) / len(gpt4_df)\n",
    "gpt3_exception_rate = (len(gpt3_df) - gpt3_non_exception_cnt) / len(gpt3_df)\n",
    "baseline_exception_rate = (len(baseline_df) - baseline_non_exception_cnt) / len(baseline_df)\n",
    "\n",
    "print(f\"Baseline Exception Rate: {round(baseline_exception_rate, 4)}\")\n",
    "print(f\"GPT3 Exception Rate: {round(gpt3_exception_rate, 4)}\")\n",
    "print(f\"GPT4 Exception Rate: {round(gpt4_exception_rate, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct outside of exception\n",
    "gpt4_non_exception_correct = gpt4_df[gpt4_df.exception == '']['ts_correct'].mean()\n",
    "gpt3_non_exception_correct = gpt3_df[gpt3_df.exception == '']['ts_correct'].mean()\n",
    "baseline_non_exception_correct = baseline_df[baseline_df.exception == '']['ts_correct'].mean()\n",
    "\n",
    "print(f\"Baseline Non-Exception Correct: {round(baseline_non_exception_correct, 4)}\")\n",
    "print(f\"GPT3 Non-Exception Correct: {round(gpt3_non_exception_correct, 4)}\")\n",
    "print(f\"GPT4 Non-Exception Correct: {round(gpt4_non_exception_correct, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check relative performance of gpt3 and gpt4 for different prompts\n",
    "gpt4_df[gpt4_df.exception == ''].groupby('prompt_name')['ts_correct'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_df[gpt3_df.exception == ''].groupby('prompt_name')['ts_correct'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot performance of gpt4 against mapped_difficulty\n",
    "display(gpt4_df[gpt4_df.exception == ''].groupby('mapped_difficulty')['ts_correct'].mean().to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_df[gpt4_df.exception == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_df[gpt4_df.exception == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_gpt4 = unified_df[['problem_difficulty', 'mapped_difficulty', 'cf_points', 'cf_rating', 'time_limit_nsec', 'memory_limit_bytes', 'model', 'prompt_name']]\n",
    "# change model and prompt_name to an integer\n",
    "corr_gpt4.loc[:,'model_2']= corr_gpt4['model'].astype('category').cat.codes\n",
    "corr_gpt4['prompt_name_2'] = corr_gpt4['prompt_name'].astype('category').cat.codes\n",
    "corr_gpt4 = corr_gpt4.drop(columns=['model', 'prompt_name'])\n",
    "\n",
    "corr_gpt4.corrwith(gpt4_df['ts_correct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make model and prompt_name into integers\n",
    "corr_gpt4['model'] = corr_gpt4['model'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "\n",
    "\n",
    "diff_vals = gpt4_df[gpt4_df.exception == ''].groupby('mapped_difficulty')['ts_correct'].mean()\n",
    "plt.bar(diff_vals.index, diff_vals.values)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code_prompting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
